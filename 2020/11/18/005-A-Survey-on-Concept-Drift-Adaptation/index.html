<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>005_A_Survey_on_Concept_Drift_Adaptation | Hexo</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="概念漂移适应研究 A Survey on Concept Drift Adaptation 当输入数据和目标变量之间的关系随时间变化时，概念漂移主要是指在线监督学习场景。假设本文具有监督学习的一般知识，我们将描述自适应学习过程的特征。对处理概念漂移的现有策略进行分类；概述最有代表性，独特和流行的技术和算法；讨论自适应算法的评估方法；并展示了一组说明性应用。这项调查以综合的方式涵盖了概念漂移的不同方">
<meta property="og:type" content="article">
<meta property="og:title" content="005_A_Survey_on_Concept_Drift_Adaptation">
<meta property="og:url" content="http://example.com/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="概念漂移适应研究 A Survey on Concept Drift Adaptation 当输入数据和目标变量之间的关系随时间变化时，概念漂移主要是指在线监督学习场景。假设本文具有监督学习的一般知识，我们将描述自适应学习过程的特征。对处理概念漂移的现有策略进行分类；概述最有代表性，独特和流行的技术和算法；讨论自适应算法的评估方法；并展示了一组说明性应用。这项调查以综合的方式涵盖了概念漂移的不同方">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2020-11-18T08:40:55.000Z">
<meta property="article:modified_time" content="2020-11-27T02:29:31.426Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-11-27 10:29:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">15</div></a></div></div></div><hr/></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Hexo</a></span><span id="menus"><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">005_A_Survey_on_Concept_Drift_Adaptation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-11-18T08:40:55.000Z" title="Created 2020-11-18 16:40:55">2020-11-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-11-27T02:29:31.426Z" title="Updated 2020-11-27 10:29:31">2020-11-27</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>概念漂移适应研究 A Survey on Concept Drift Adaptation</p>
<p>当输入数据和目标变量之间的关系随时间变化时，概念漂移主要是指在线监督学习场景。假设本文具有监督学习的一般知识，我们将描述自适应学习过程的特征。对处理概念漂移的现有策略进行分类；概述最有代表性，独特和流行的技术和算法；讨论自适应算法的评估方法；并展示了一组说明性应用。这项调查以综合的方式涵盖了概念漂移的不同方面，以反思现有的现有分散状态。因此，它旨在为研究人员，行业分析人员和从业人员全面介绍概念漂移适应。 Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.</p>
<h2 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h2><p>我们的数字世界正在迅速增长。根据IDC调查报告，2012年生成的数据量估计超过2.8 zbytes（2.8万亿千兆字节）[Gantz and Reinsel 2012]。迫切需要有效，有效的工具和分析方法来处理不同应用程序和领域中不断增长的数据量。传统上，在数据挖掘中，已收集的数据以脱机模式进行处理。例如，使用作为一组对（输入，输出）给出的历史数据来训练预测模型。随后可以将以这种方式训练的模型应用于预测新的看不见的输入数据的输出。 Our digital universe is rapidly growing. The volume of data generated in 2012 has been estimated to surpass 2.8 zetabytes (2.8 trillion gigabytes) as reported in the IDC survey [Gantz and Reinsel 2012]. Efficient and effective tools and analysis methods for dealing with the ever-growing amount of data in different applications and fields are of paramount need. Traditionally in data mining, already collected data is processed in an offline mode. For instance, predictive models are trained using historical data given as a set of pairs (input, output). Models trained in such a way can be afterwards applied for predicting the output for new unseen input data.</p>
<p>但是，数据经常以流的形式出现。在机器的主内存中容纳大量流数据是不切实际的，而且通常是不可行的。因此，仅在线处理是合适的。在这种情况下，可以通过连续更新或通过使用最近一批数据进行重新训练来增量地训练预测模型。但是，计算效率并不是监督数据流学习的唯一问题。 However , very often data comes in the form of streams. Accommodating large volumes of streaming data in the machine’s main memory is impractical and often infeasible. Hence, only an online processing is suitable. In this case, predictive models can be trained either incrementally by continuous update or by retraining using recent batches of data. But computational efficiency is not the only issue in supervising learning from data steams.</p>
<p>在动态变化和非平稳的环境中，数据分布会随着时间而变化，从而产生概念漂移现象[Schlimmer and Granger 1986; Widmer and Kubat 1996]。实际概念漂移是指在给定输入（输入要素）的情况下输出（即目标变量）的条件分布的变化，而输入的分布可能保持不变。真实概念漂移的一个典型示例是关注在线新闻流时用户兴趣的变化。尽管传入新闻文档的分发通常保持不变，但该用户感兴趣（因此不感兴趣）的新闻文档的条件分发会发生变化。<strong>自适应学习是指在预测模型运行过程中在线更新它们以对概念漂移做出反应</strong>。 In dynamically changing and nonstationary environments, the data distribution can change over time, yielding the phenomenon of concept drift [Schlimmer and Granger 1986; Widmer and Kubat 1996]. The real concept drift refers to changes in the conditional distribution of the output (i.e., target variable) given the input (input features), while the distribution of the input may stay unchanged. A typical example of the real concept drift is a change in users’ interests when following an online news stream. While the distribution of the incoming news documents often remains the same, the conditional distribution of the interesting (and thus not interesting) news documents for that user changes. <strong>Adaptive learning refers to updating predictive models online during their operation to react to concept drifts.</strong></p>
<p>在过去的十年中，与概念漂移学习相关的研究日益增长，并且已经开发了许多漂移感知自适应学习算法。尽管该研究主题很受欢迎，但社区尚未获得有关概念漂移处理技术的全面调查。造成这种情况的原因之一是，该问题的范围很广，涉及不同的研究领域。而且，术语还不完善。因此，已经在不同的环境下以不同的名称独立开发了类似的自适应学习策略。 Over the last decade, research related to learning with concept drift has been increasingly growing, and many drift-aware adaptive learning algorithms have been developed. In spite of the popularity of this research topic, no comprehensive survey on concept drift handling techniques is available to the community . One of the reasons for this is that the problem is of a wide scope and spans across different research fields. Moreover , terminology is not well established; thus, similar adaptive learning strategies have been developed independently under different names in different contexts.</p>
<p>考虑到当前关于概念漂移的研究的现状，这种概念非常流行，而且散布在各个社区中，因此迫切需要对迄今为止所做的研究进行全面总结，以统一研究人员中的概念和术语并调查研究人员。过去研究过的最先进的方法和技术。 Taking account of the current picture of research on concept drift, being very popular but also scattered among various communities, there is a strong need for a comprehensive summary of the research done so far to unify the concepts and terminology among the researchers and to survey the state-of-the-art methodologies and techniques investigated in the past.</p>
<p>提供了一些与漂移意识学习相关的评论。<strong>但是，它们要么不仅仅专注于概念漂移，要么不涉及自适应学习的特定主题。因此，这些评论是零散的和/或过时的。</strong>目前，关于概念漂移的引用最多的调查早于2004年在Tsymbal [2004]中发表。以下与概念漂移主题相关的概述主要集中于<strong>集成技术[Kuncheva 2004，2008]，归纳规则学习算法[Maloof 2010]或主要是非增量学习技术[Zliobaite 2009]，它们可以不受限制地使用计算资源，</strong>因此范围有限。对数据流的评论[Gaber等。 2005; Gama 2010； Bifet等。 2011]仅部分处理数据漂移问题。<strong>数据流研究仅在某种程度上涵盖了自适应学习，而主要重点仍然是使学习算法递增并优化计算资源和预测精度之间的平衡。</strong> Several reviews related to drift-aware learning are available. However , they either do not focus exclusively on concept drift or relate to specific topics of adaptive learning. Thus, these reviews are fragmented and/or outdated. Currently , the most cited survey on concept drift was published back in 2004 in Tsymbal [2004]. The following overviews, which are related to the topic of concept drift focused on ensemble techniques [Kuncheva 2004, 2008], inductive rule learning algorithms [Maloof 2010], or mainly nonincremental learning techniques [Zliobaite 2009] that can use computational resources unrestrictedly , thus were limited in scope. Reviews on data streams [Gaber et al. 2005; Gama 2010; Bifet et al. 2011] only partially deal with data drift. <strong>Data streams research covers adaptive learning only to some extent, while the main focus remains on making learning algorithms incremental and optimizing the balance of computational resources and the predictive accuracy</strong></p>
<p>一些评论仅限于特定的应用领域。一份有针对性的立场文件[Grisogono 2006]提出了一套用于国防的复杂自适应系统的要求。最近的重点综述[Kadlec等。 [2011年]调查了用于软传感器的适应机制。<strong>最后，最近的一篇文章[Moreno-Torres等。 [2012年]专注于描述数据分布随时间变化的各种方式，并且仅从数据集转移社区的角度简要介绍了适应技术，其中大部分都忽略了概念漂移方面的工作。</strong>最近的综述[Alberg等。 [2012]专注于决策树。 Several reviews are limited to specific application fields. A focused position paper [Grisogono 2006] presents a set of requirements for complex adaptive systems to be used for defense. A recent focused review [Kadlec et al. 2011] surveys adaptation mechanisms that have been used for soft sensors. <strong>Finally , a recent article [Moreno-Torres et al. 2012] focuses on describing various ways in which data distribution can change over time and only briefly covers adaptation techniques from the dataset shift community perspective, mostly leaving out works on concept drift.</strong> A recent review [Alberg et al. 2012] focuses on decision trees.</p>
<p>本文稿<strong>通过调查自适应学习方法，介绍评估方法并讨论说明性应用程序，提供了有关处理概念漂移的综合视图</strong>。<strong>它专注于在线监督学习的当输入要素和目标变量之间的关系随时间变化</strong>，。 The present contribution <strong>provides an integrated view on handling concept drift by surveying adaptive learning methods, presenting evaluation methodologies, and discussing illustrative applications</strong>. It focuses on online supervised learning when the relation between the input features and the target variable changes over time.</p>
<p>文章的结构安排如下。在第2节中，我们介绍了<strong>概念漂移问题，描述了自适应学习算法的特点</strong>，并给出了具有启发性的应用示例。第3节介绍了<strong>自适应学习方法的综合分类法</strong>。第四部分讨论了<strong>自适应学习算法的实验设置和评估方法</strong>。第5节总结调查 The article is organized as follows. In Section 2, we introduce the problem of concept drift, characterize adaptive learning algorithms, and present motivating application examples. Section 3 presents a comprehensive taxonomy of methods for adaptive learning. Section 4 discusses the experimental settings and evaluation methodologies of adaptive learning algorithms. Section 5 concludes the survey</p>
<h2 id="2-自适应学习算法-2-ADAPTIVE-LEARNING-ALGORITHMS"><a href="#2-自适应学习算法-2-ADAPTIVE-LEARNING-ALGORITHMS" class="headerlink" title="2.自适应学习算法 2. ADAPTIVE LEARNING ALGORITHMS"></a>2.自适应学习算法 2. ADAPTIVE LEARNING ALGORITHMS</h2><p>学习算法通常需要在动态环境中运行，动态环境正在发生意想不到的变化。这些算法的一个<strong>理想特性是它们合并新数据的能力</strong>。如果数据生成过程不是严格固定的（适用于大多数现实应用程序），<strong>那么我们预测的基本概念（例如，用户阅读新闻的兴趣）可能会随着时间而改变</strong>。适应这种概念漂移的能力可以看作是增量学习系统的自然扩展[Giraud-Carrier 2000]，它可以逐个示例地学习预测模型。<strong>自适应学习算法可以看作是高级增量学习算法，能够随着时间的推移适应数据生成过程的发展。</strong>本节介绍概念漂移并描述自适应学习的特征。 Learning algorithms often need to operate in dynamic environments, which are changing unexpectedly . One desirable property of these algorithms is their ability of incorporating new data. If the data-generating process is not strictly stationary (as applies to most of the real-world applications), the underlying concept, which we are predicting (e.g., interests of a user reading news), may be changing over time. The ability to adapt to such concept drift can be seen as a natural extension for the incremental learning systems [Giraud-Carrier 2000] that learn predictive models example by example. <strong>Adaptive learning algorithms can be seen as advanced incremental learning algorithms that are able to adapt to evolution of the data-generating process over time.</strong> This section introduces concept drift and characterizes adaptive learning.</p>
<h3 id="2-1。设置和定义-2-1-Setting-and-Definitions"><a href="#2-1。设置和定义-2-1-Setting-and-Definitions" class="headerlink" title="2.1。设置和定义 2.1. Setting and Definitions"></a>2.1。设置和定义 2.1. Setting and Definitions</h3><p>在机器学习中，监督学习问题的形式如下。我们的目标是在给定一组输入特征 X 的情况下，在回归任务（或分类任务中的y）中预测目标变量y。一个n例子是一对（X，y）。例如，X是下午2点化学过程的一组传感器读数。 1月2日，并且y =“好”是当时所生产产品的真实质量。在用于模型构建的训练示例中，X和y都是已知的。在应用了预测模型的新示例中，Xis已知，但是在预测时y未知 In machine learning, the supervised learning problem is formally defined as follows. We aim to predict a target variable y ∈ ?1in regression tasks (or y categorical in classification tasks) given a set of input features X ∈ ?p. A n example is one pair of (X, y). For instance, X is a set of sensor readings of a chemical process at 2 p.m. on the 2ndof January and y = “good” is the true quality of the produced product at that time. In the training examples, which are used for model building, both X and y are known. In the new examples, on which the predictive model is applied, Xis known, but y is not known at the time of prediction</p>
<p>根据贝叶斯决策理论[Duda等。 [2001]，可以通过<strong>类别p（y）的先验概率</strong>和<strong>类别条件概率密度函数p（X | y）</strong>来描述所有类别y = 1，…，c的分类，其中c是数字类。<strong>根据类别的后验概率做出分类决策</strong>，对于类别y可以表示为</p>
<p>$$p(y \mid X)=\frac{p(y) p(X \mid y)}{p(X)}$$</p>
<p> According to the Bayesian Decision Theory [Duda et al. 2001], a classification can be described by the prior probabilities of the classes p(y) and the class conditional probability density functions p(X|y) for all classes y = 1,…,c, where c is the number of classes. The classification decision is made according to the posterior probabilities of the classes, which for class y can be represented as</p>
<p>此处假定错误分类的成本相等。 Here equal costs of misclassification are assumed.</p>
<p>目标变量空间的类型取决于任务。在分类中，目标变量采用分类值（类标签），而在回归中，目标变量采用连续值。 The type of the target variable space depends on the task. In classification, the target variable takes categorical values (class labels), while in regression, the target variable takes continuous values.</p>
<p>我们可以区分两种学习模式：<strong>离线学习和在线学习</strong>。在离线学习中，整个训练数据必须在模型训练时可用。只有训练完成后，才能将模型用于预测。相反，在线算法按顺序处理数据。<strong>他们生成了一个模型并将其投入运行，而一开始没有完整的训练数据集</strong>。随着更多训练数据的到来，该模型会在操作过程中不断更新。 We can distinguish two learning modes:offlinelearning andonlinelearning. In offline learning, the whole training data must be available at the time of model training. Only when training is completed can the model be used for predicting. In contrast, online algorithms process data sequentially . <strong>They produce a model and put it in operation without having the complete training dataset available at the beginning.</strong> The model is continuously updated during operation as more training data arrives.</p>
<p>增量算法比在线算法的限制性要小，它们可以一个接一个地处理输入示例（或逐批处理），并在收到每个示例后更新决策模型。增量算法可以随机访问先前的示例或代表/选定的示例。在这种情况下，这些算法称为带有部分内存的增量算法[Maloof和Michalski 2004]。通常<strong>，在增量算法中，对于任何新的数据表示形式，模型的更新操作均基于上一个模型。流算法是用于处理高速连续数据流的在线算法。</strong>在流式传输中，示例也将按顺序处理，并且仅需几次检查即可（通常只有一次）。这些算法使用有限的内存和每个项目有限的处理时间。 Less restrictive than online algorithms areincrementalalgorithms that process input examples one by one (or batch by batch) and update the decision model after receiving each example. Incremental algorithms may have random access to previous examples or representative/selected examples. In such a case, these algorithms are called incremental algorithms with partial memory [Maloof and Michalski 2004]. Typically , in incremental algorithms, for any new presentation of data, the update operation of the model is based on the previous one. Streaming algorithms are online algorithms for processing high-speed continuous flows of data. In streaming, examples are processed sequentially as well and can be examined in only a few passes (typically just one). These algorithms use limited memory and limited processing time per item.</p>
<p>在我们考虑的情况下，数据通常实时地在线到达，从而形成了可能无限的数据流。为机器提供了刚到达的输入数据，目的是预测其目标变量的值。<strong>机械必须构造一个预测模型，该模型是输入（特征）空间与其对应的输出（目标）空间之间的映射函数</strong>。例如，给定化学生产过程中的传感器读数，任务是预测产品（输出）的质量。 In the setting that we are considering, data arrives online, often in real time, forming a stream that is potentially infinite. The machinery is given input data that has just arrived with the goal of predicting the value of its target variable(s). The machinery must construct a predictive model that is a mapping function between the input (feature) space to its corresponding output (target) space. For instance, given sensor readings in a chemical production process, the task is to predict the quality of the product (output).</p>
<p>因为预计数据会随着时间的推移而发展，尤其是在动态变化的环境（通常是非平稳性）中，所以其基本分布会随着时间动态变化。概念漂移设置中的<strong>一般假设是，更改会意外发生且不可预测</strong>，尽管在某些特定的实际情况下，可以与特定环境事件的发生提前得知更改。但是，针对一般漂移的解决方案需要针对特定情况的解决方案。而且，改变可以采取不同的形式（即，输入数据特性或输入数据与目标变量之间的关系可以改变）。 Because data is expected to evolve over time, especially in dynamically changing environments, where nonstationarity is typical, its underlying distribution can change dynamically over time. The general assumption in the concept drift setting is that the change happens unexpectedly and is unpredictable, although in some particular real-world situations, the change can be known ahead of time in correlation with the occurrence of particular environmental events. But solutions for the general case of drift entail the solutions for the particular cases. Moreover , the change may take different forms (i.e., the input data characteristics or the relation between the input data and the target variable may change).</p>
<p>形式上，时间点$t_0$和时间点$t_1$之间的概念漂移可以定义为</p>
<p>$$\exists X: p_{t_{0}}(X, y) \neq p_{t_{1}}(X, y)$$$</p>
<p>其中pt0表示在时间t0处输入变量X与目标变量y之间的联合分布。数据的变化可以被描述为这种关系的组成部分的变化[Kelly等。 1999年；高等。 2007]。换句话说：</p>
<p>-类p（y）的先验概率可能会发生变化；</p>
<p>-类条件条件概率p（X | y）可能会发生变化；以及</p>
<p>-结果，类p（y | X）的后验概率可能会发生变化，影响预测。 where pt0denotes the joint distribution at time t0between the set of input variables X and the target variable y. Changes in data can be characterized as changes in the components of this relation [Kelly et al. 1999; Gao et al. 2007]. In other terms: —the prior probabilities of classes p(y) may change, —the class conditional probabilities p(X|y) may change, and —as a result, the posterior probabilities of classes p(y|X) may change, affecting the prediction.</p>
<p>我们有兴趣了解这些变化的两个含义：（i）<strong>数据分布p（y | X）是否发生变化并影响预测决策；</strong>（ii）<strong>在不知道真实标签的情况下，数据更改是否可见于数据分布（即，p（X）更改</strong>）。从预测的角度来看，只有影响预测决策的更改才需要调整。我们可以区分两种类型的漂移： We are interested to know two implications of these changes: (i) whether the data distribution p(y|X) changes and affects the predictive decision and (ii) whether the changes are visible from the data distribution without knowing the true labels (i.e., p(X) changes). From a predictive perspective, only the changes that affect the prediction decision require adaptation. We can distinguish two types of drifts:</p>
<p>（1）<strong>实际概念漂移是指p（y | X）的变化。</strong>无论p（X）有无变化，这种变化都可能发生。实际的概念漂移在Salganicoff [1997]中被称为概念漂移，在Gao等人中被称为条件变化。 [2007]。 </p>
<p>（2）**如果传入数据的分布发生变化（即p（X）发生变化）而又不影响p（y | X），则会发生虚拟漂移[**Delany等。 2005; Tsymbal 2004； Widmer and Kubat 1993]。但是，虚拟漂移在文献中有不同的解释：</p>
<p>—最初，虚拟漂移被定义为[Widmer and Kubat 1993]是由于数据表示不完整而不是实际概念的变化而发生的。</p>
<p><strong>虚拟漂移与数据分布的变化相对应，导致决策边界的变化</strong>[Tsymbal 2004]。</p>
<p>虚拟漂移是一种不影响目标概念的漂移[Delany等。 2005]。 (1) Real concept drift refers to changes in p(y|X). Such changes can happen either with or without change in p(X). Real concept drift has been referred to as concept shift in Salganicoff [1997] and conditional change in Gao et al. [2007]. (2) Virtual drift happens if the distribution of the incoming data changes (i.e., p(X) changes) without affecting p(y|X) [Delany et al. 2005; Tsymbal 2004; Widmer and Kubat 1993]. However , virtual drift has had different interpretations in the literature: —Originally , a virtual drift was defined [Widmer and Kubat 1993] to occur due to incomplete data representation rather than change in concepts in reality . —Virtual drift corresponds to change in data distribution that leads to changes in the decision boundary [Tsymbal 2004]. —Virtual drift is a drift that does not affect the target concept [Delany et al. 2005].</p>
<p>虚拟漂移也被称为临时漂移[Lazarescu等。 2004]，采样偏移[Salganicoff 1997]和特征变化[Gao等。 2007]。</p>
<p>在本文中，虚拟漂移是指数据分布p（X）的变化 —Virtual drift has been also referred to as temporary drift [Lazarescu et al. 2004], sampling shift [Salganicoff 1997], and feature change [Gao et al. 2007]. In this article, virtual drift refers to change in the data distribution p(X)</p>
<p>示例：考虑有关房地产的在线新闻流。<strong>给定用户的任务是将传入新闻分类为相关新闻和不相关新闻。</strong>假设用户正在寻找新公寓；则<strong>有关民居的新闻是相关的，而度假屋则无关</strong>。如果新闻门户的编辑者发生了变化，写作风格也将发生变化，但是住宅仍然与用户相关。这种情况对应于<strong>虚拟漂移</strong>。如果由于危机而发表了更多关于住宅的文章，而发表了更少的关于度假屋的文章，但编辑，写作风格和用户兴趣保持不变，则这种情况<strong>对应于类别的先验概率出现偏差</strong>。另一方面，如果用户已经买了房子并开始寻找度假目的地，则住宅变得不相关而度假屋变得相关。这种情况对应于<strong>实际概念漂移</strong>。在这种情况下，写作风格和先验概率保持不变。所有类型的漂移可能会同时发生。 Example: Consider an online news stream of articles on real estate. The task for a given user is to classify the incoming news into relevant and not relevant. Suppose that the user is searching for a new apartment; then news on dwelling houses is relevant, whereas holiday homes are not relevant. If the editor of the news portal changes, the writing style changes as well, but the dwelling houses remain relevant for the user . This scenario corresponds to virtual drift. If, due to a crisis, more articles on dwelling houses come out and fewer articles on holiday homes do but the editor , the writing style, and the interests of the user remain the same, this situation corresponds to drift in prior probabilities of the classes. If, on the other hand, the user has bought a house and starts looking for a holiday destination, dwelling houses become not relevant and holiday homes become relevant. This scenario corresponds to the real concept drift. In this case, the writing style and the prior probabilities stay the same. It may happen that all types of drifts may take place at the same time.</p>
<p>图1说明了漂移的类型。该图显示，只有实际概念漂移会更改类边界，并且先前的决策模型已过时。<strong>实际上，改变实际先验概率或新颖性的虚拟漂移可能与实际漂移结合出现。在这些情况下，类别边界也会受到影响</strong>。 Figure 1 illustrates the types of drifts. The plot shows that only the real concept drift changes the class boundary and the previous decision model becomes obsolete. In practice, virtual drift changing prior probabilities or novelties may appear in combination with the real drift. In these cases, the class boundary is also affected.</p>
<p>该调查主要关注于<strong>处理实际概念漂移，这在输入数据分布中是不可见的。</strong>在许多情况下，处理实际概念漂移的技术还可以处理在输入数据分布中表现出来的漂移，但反之则不行。用于<strong>处理实际概念漂移的技术通常依赖于有关预测性能的反馈，</strong>而用于跟踪变化的先验概率的技术和用于处理虚拟漂移或新颖性检测的技术通常可以在没有此类反馈的情况下运行。本文<strong>没有涵盖可以从传入数据分布（P（X））中检测到的漂移</strong>。对跟踪漂移先验概率感兴趣的读者请参见Zhang和Zhou [2010]，新颖性检测请参见Markou和Singh [2003]和Masud等。 [2011]，以及在使用基于聚类的半监督学习技术处理虚拟漂移时，请参考Aggarwal [2005]，Bouchachia等。 [2010]，以及Bouchachia和Vanaret [2013]。 This survey primarily focuses on handling the real concept drift, which is not visible from the input data distribution. In many cases, the techniques that handle the real concept drift can also handle drifts that manifest in the input data distributions, but not the other way around. Techniques for handling the real concept drift typically rely on feedback about the predictive performance, while techniques for tracking changing prior probabilities and techniques for handling virtual drift or novelty detection typically can operate without such feedback. This article does not cover such drifts that can be detected from the incoming data distribution (P(X)). Readers interested in tracking drifting prior probabilities are referred to Zhang and Zhou [2010], in novelty detection are referred to Markou and Singh [2003] and Masud et al. [2011], and in handling virtual drifts by semisupervised learning techniques using on clustering are referred to Aggarwal [2005], Bouchachia et al. [2010], and Bouchachia and Vanaret [2013].</p>
<h3 id="2-2。数据随时间变化-2-2-Changes-in-Data-Over-Time"><a href="#2-2。数据随时间变化-2-2-Changes-in-Data-Over-Time" class="headerlink" title="2.2。数据随时间变化 2.2. Changes in Data Over Time"></a>2.2。数据随时间变化 2.2. Changes in Data Over Time</h3><p><strong>数据分布随时间的变化可能以不同的形式体现出来</strong>，如图2所示，是玩具的一维数据。<strong>在此数据中，数据均值发生变化</strong>。通过从一个概念切换到另一个概念（例如，用化工厂中具有不同校准值的另一个传感器替换传感器），<strong>可能突然/突然发生漂移</strong>，或者由介于两者之间的许多中间概念组成（例如，传感器）慢慢磨损并变得不太准确）。<strong>漂移可能突然发生</strong>（例如，作为信用分析员正在调查的感兴趣的主题可能突然从例如肉类价格转变为公共交通）<strong>或逐渐发生</strong>（例如，相关新闻主题从住宅变为度假屋，而用户不会突然切换，而是会在一段时间内回到先前的兴趣）。<strong>概念漂移处理算法的挑战之一是不要将真正的漂移与离群值或噪声混合在一起，后者是指一次性的随机偏差或异常</strong>（离群值检测请参见Chandola等人[2009]）。在后一种情况下，不需要适应性。最后，<strong>漂移可能会引入以前未见过的新概念，或者一段时间后（例如，以时尚的形式）可能会再次出现以前见过的概念</strong>。变化可以通过<strong>严重性，可预测性和频率来进一步表征</strong>[Minku等。 2010; Kosina等。 2010]。 Changes in data distribution over time may manifest in different forms, as illustrated in Figure 2 on a toy one-dimensional data. <strong>In this data, changes happen in the data mean</strong>. A drift may happen <strong>suddenly/abruptly</strong>,by switching from one concept to another (e.g., replacement of a sensor with another sensor that has a different calibration in a chemical plant), <strong>or incrementally, consisting of many intermediate concepts in between</strong> (e.g., a sensor slowly wears off and becomes less accurate). Drift may happen suddenly (e.g., the topics of interest that one is surveying as a credit analyst may suddenly switch from, for instance, meat prices to public transportation) or gradually (e.g., relevant news topics change from dwelling to holiday homes, while the user does not switch abruptly , but rather keeps going back to the previous interest for some time). One of the challenges for concept drift handling algorithms is not to mix the true drift with an outlier or noise, which refers to a once-off random deviation or anomaly (see Chandola et al. [2009] for outlier detection). No adaptivity is needed in the latter case. Finally , drifts may introduce new concepts that were not seen before, or previously seen concepts may reoccur after some time (e.g., in fashion). Changes can be further characterized by severity , predictability , and frequency [Minku et al. 2010; Kosina et al. 2010].</p>
<p>大多数自适应学习技术都<strong>隐式或显式地假定并专门研究概念漂移的某些子集</strong>。他们中的许多人都<strong>假设突然发生了不可重复的漂移</strong>。但实际上，通常可以观察到多种类型的混合物。 Most of the adaptive learning techniques implicitly or explicitly assume and specialize in some subset of concept drifts. Many of them assume sudden nonreoccurring drifts. But in reality , often mixtures of many types can be observed.</p>
<h3 id="2-3。不断变化的环境中对预测模型的要求-2-3-Requirements-for-Predictive-Models-in-Changing-Environments"><a href="#2-3。不断变化的环境中对预测模型的要求-2-3-Requirements-for-Predictive-Models-in-Changing-Environments" class="headerlink" title="2.3。不断变化的环境中对预测模型的要求 2.3. Requirements for Predictive Models in Changing Environments"></a>2.3。不断变化的环境中对预测模型的要求 2.3. Requirements for Predictive Models in Changing Environments</h3><p>在这些设置下运行的预测模型需要具有机制，可以随着时间的推移检测和适应不断发展的数据。否则，它们的准确性将会降低。随着时间的流逝，可能需要考虑新数据来更新决策模型，或者完全替换决策模型以满足变化的情况。需要预测模型以：</p>
<p>（1）<strong>尽快检测</strong>概念漂移（并在需要时进行<strong>调整</strong>）； </p>
<p>（2）<strong>区分</strong>漂移和<strong>噪声</strong>，并能适应变化，但对<strong>噪声具有鲁棒性</strong>；</p>
<p>（3）在少于示例<strong>到达时间</strong>的情况下运行，并且任何存储使用<strong>不超过固定数量的内存</strong>。</p>
<p>Predictive models that operate in these settings need to have mechanisms to detect and adapt to evolving data over time; otherwise, their accuracy will degrade. As time passes, the decision model may need to be updated taking into account the new data or be completely replaced to meet the changed situation. Predictive models are required to: (1) detect concept drift (and adapt if needed) as soon as possible; (2) distinguish drifts from noise and be adaptive to changes, but robust to noise; and (3) operate in less than example arrival time and use not more than a fixed amount of memory for any storage.</p>
<h3 id="2-4。在线自适应学习程序"><a href="#2-4。在线自适应学习程序" class="headerlink" title="2.4。在线自适应学习程序"></a>2.4。在线自适应学习程序</h3><p>在线自适应学习的正式定义如下。决策模型是将输入变量映射到目标的函数L：y = L（X）。学习算法指定了如何从一组数据实例构建模型。在线自适应学习程序如下：</p>
<p>（1）预测。当新的示例$X_t$到达时，使用当前模型$L_{t}$.</p>
<p>（2）诊断。一段时间后，我们收到真实标签$y_{t}$，并且可以将损失估计为$f\left(\hat{y}<em>{t}, y</em>{t}\right)$</p>
<p>（3）更新。我们可以使用示例$({X}<em>{t}, y</em>{t})$进行模型更新以获得$L_{t + 1}$。 2.4. Online Adaptive Learning Procedure<br>The online adaptive learning is formally defined as follows. A decision model is a function L that maps the input variables to the target: y = L(X). A learning algorithm specifies how to build a model from a set of data instances. The online adaptive learning procedure is the following: (1) Predict. When new example Xtarrives, a prediction ˆ ytis made using the current model Lt. (2) Diagnose. After some time, we receive the true label ytand can estimate the loss as f(ˆ yt, yt). (3) Update. We can use the example (Xt, yt) for the model update to obtain Lt+1.</p>
<p>$\mathcal{L}<em>{t+1}=\operatorname{train}\left(\left(X</em>{i}, y_{i}\right), \ldots,\left(X_{t}, y_{t}\right), \mathcal{L}_{t}\right)$</p>
<p>根据计算资源，一旦使用<strong>模型$\mathcal{L}<em>{t+1}=\operatorname{train}\left(\left(X</em>{t}, y_{t}\right), \mathcal{L}_{t}\right) .$的最新</strong>版本进行处理，则可能需要丢弃数据。可选地，一些过去的数据可以保持可访问性$\mathcal{L}<em>{t+1}=\operatorname{train}\left(\left(X</em>{i}, y_{i}\right), \ldots,\left(X_{t}, y_{t}\right), \mathcal{L}_{t}\right)$。有多种在线处理数据的方式（例如，<strong>部分内存</strong>：<strong>一些示例</strong>在培训中<strong>定期存储和使用</strong>;基于<strong>窗口</strong>的数据以<strong>块</strong>的形式显示;基于<strong>实例</strong>的示例在<strong>到达时进行处理</strong>）。与数据表示相关的各种方案的更多详细信息将在第3节中介绍。 Depending on the computational resources, the data may need to be discarded once processed using the latest version of the model Lt+1= train((Xt, yt),Lt). Alternatively , some of the past data may remain accessible Lt+1= train((Xi, yi),…,(Xt, yt),Lt). There are different ways of handling data online (e.g., partial memory: some of the examples are stored and used regularly in the training; window based: data is presented as chunks; instance based: an example is processed upon its arrival). More details on various schemes related to data presentation will follow in Section 3.</p>
<p>更新模型后，新示例Xt + 1到达，接收预测反馈模型更新的循环将无限期继续。在某些时间步长，可以选择保留当前模型Lt + 1 =Lt。 After updating the model, new example Xt+1arrives and the loop of receivingpredicting-feedback-model update continues infinitely. At some time steps, one may choose to preserve the current model Lt+1= Lt.</p>
<p>图3描绘了在线自适应学习算法的通用架构。简而言之，存储模块定义了<strong>如何将数据</strong>以及<strong>哪些数据</strong>呈现给学习算法（学习模块）。损失估计模块<strong>跟踪学习算法的性能</strong>，并在必要时将信息发送到<strong>更改检测模块</strong>以<strong>更新模型</strong>。第三部分将详细讨论系统的四个模块（内存，学习，更改检测，损失估计）。 Figure 3 depicts a generic schema for an online adaptive learning algorithm. In a nutshell, the memory module defines how and which data is presented to the learning algorithm (learning module). The loss estimation module tracks the performance of the learning algorithm and sends information to the change detection module to update the model if necessary . Section 3 will discuss the four modules of the system (memory , learning, change detection, loss estimation) in detail.</p>
<p>此设置有一些变化，例如，目标变量（反馈）的<strong>真实值会延迟或根本不可用</strong>。此外，在我们获得对已处理数据的<strong>反馈之前</strong>，可能会<strong>出现新的预测示例</strong>。在这种情况下，模型<strong>更新将被延迟</strong>，但是操作原理保持不变。最后，在某些情况下，我们可能需要<strong>分批</strong>处理示例，而不是一个接一个地处理示例。<br>        This setting has variations where, for instance, the true values for the target variable (feedback) come with a delay or are not available at all. Moreover , new examples for prediction may arrive before we get feedback for the data that has already been processed. In such a case, model update would be delayed, but the principles of operation remain the same. Finally , in some settings, we may need to process examples in batches rather than one by one.</p>
<h3 id="2-5。说明性应用"><a href="#2-5。说明性应用" class="headerlink" title="2.5。说明性应用"></a>2.5。说明性应用</h3><p>​        2.5. Illustrative Applications</p>
<p>概念漂移问题已在包括医学在内的多个领域和应用领域得到认可和解决[Kukar 2003; Tsymbal等。 2006]，工业[Pechenizkiy等。 2009]，教育[Castillo等。 2003]和业务[Klinkenberg 2003]。需要适应的应用程序可以分为四类：</p>
<p><strong>监视和控制</strong>。此类别包括检测网络，计算机网络，电信，金融交易以及其他需要发出异常行为的应用程序区域中的<strong>异常行为</strong>和<strong>对手活动</strong>，通常将其制定为检测任务。</p>
<p><strong>管理</strong>和<strong>战略规划</strong>。此类别包括<strong>预测分析任务</strong>，例如，按地区评估信用度，需求预测，食品销售，公交时间预测和犯罪预测。</p>
<p><strong>个人协助</strong>和信息。此类别包括<strong>推荐系统</strong>，文本信息的分类和组织，用于营销的客户分析，个人邮件分类和<strong>垃圾邮件过滤</strong>。</p>
<p>无处不在的<strong>环境应用程序</strong>。此类别包括各种各样的<strong>移动和固定系统</strong>，它们与不断变化的环境相互作用，例如移动机器人，移动车辆和智能家用电器。<br>        The problem of concept drift has been recognized and addressed in multiple domains and application areas, including medicine [Kukar 2003; Tsymbal et al. 2006], industry [Pechenizkiy et al. 2009], education [Castillo et al. 2003], and business [Klinkenberg 2003]. Applications requiring adaptation can be grouped into four categories: Monitoring and control. This category includes detection of anomalous behavior and adversary activities on the web, computer networks, telecommunications, financial transactions, and other application areas where an abnormal behavior needs to be signaled, and it is often formulated as a detection task. Management and strategic planning. This category includes predictive analytics tasks, such as evaluation of creditworthiness, demand prediction, food sales, bus travel time prediction, and crime prediction by region. Personal assistance and information. This category includes recommender systems, categorization and organization of textual information, customer profiling for marketing, personal mail categorization, and spam filtering. Ubiquitous environment applications. This category includes a wide spectrum of moving and stationary systems, which interact with changing environments, for instance, moving robots, mobile vehicles, and smart household appliances.</p>
<p>接下来，我们讨论每个类别中的<strong>激励应用案例</strong>，以说明可以处理概念漂移的自适应学习系统的需求。<br>        Next we discuss motivating application cases within each category to illustrate the demand of adaptive learning systems that can handle concept drift.</p>
<p>2.5.1。监控。在监视和控制应用中，数据通常以时间序列的形式呈现。两个最典型的学习任务是时间序列预测（回归任务）或异常检测（分类任务）。<br>        2.5.1. Monitoring and Control. In monitoring and control applications, data is often presented in a form of time series. The two most typical learning tasks are time-series forecasting (regression task) or anomaly detection (classification task).</p>
<p>工业锅炉中的在线质量流量预测[Pechenizkiy等。 2009]是监视和控制类别中的示例应用程序。质量流量预测将有助于改善锅炉的运行和控制。在稳定运行中，燃烧受到燃料进料速率的干扰以及床中燃料不完全混合的影响。知道质量流量对于锅炉控制很重要。该系统吸收在内部连续混合并从容器转移到锅炉的燃料。位于容器下方的缩放传感器可提供流数据。任务是实时预测（估计）质量流量。<br>        Online mass flow prediction in an industrial boiler [Pechenizkiy et al. 2009] is an example application in the monitoring and control category . Mass flow prediction would help to improve operation and control of the boiler . In steady operation, combustion is affected by the disturbances in the feed rate of the fuel and by the incomplete mixing of the fuel in the bed. Knowing the mass flow is important for boiler control. The system takes fuel that is continuously mixed inside and transferred from a container to the boiler . Scaling sensors located under the container provide streaming data. The task is to predict (estimate) the mass flow in real time.</p>
<p>由于以下原因，会发生概念漂移。供油是一种手动且非标准化的过程，不一定平稳，并且可能会短暂中断。每个操作员都有不同的习惯。过程特性可能取决于所用燃料的类型和质量。自适应学习算法的主要焦点是处理两种类型的变化：突然改变为进给和较慢但仍然突然转变为燃烧。学习的挑战之一是根本无法获得反馈（质量流的基本事实）。它只能通过回顾性检查历史数据来近似估计。另一个挑战是处理容易被误认为更改的特定的单方面离群值。<br>        Concept drift happens due to the following reasons. Fuel feeding is a manual and nonstandardized process, which is not necessarily smooth and can have short interruptions. Each operator can have different habits. The process characteristics may depend on the type and the quality of fuel used. The main focus for an adaptive learning algorithm is to handle two types of changes: an abrupt change to feeding and slower but still abrupt switch to burning. One challenge for learning is that the feedback (the ground truth of mass flow) is not available at all; it can only be approximately estimated by retrospectively inspecting the historical data. An additional challenge is to deal with specific one-sided outliers that can be easily mistaken for changes.</p>
<p>传统的方法（例如ADWIN；请参阅第3.2.3节）基于监视原始传感器信号或回归变量的流错误进行显式更改检测，可以得出合理的结果。通过考虑应用程序的特殊性可以改善它们。<br>        Traditional approaches (such as ADWIN; see Section 3.2.3) for explicit change detection based on the monitoring of the raw sensor signal or streaming error of the regressors give reasonable results. They can be improved by considering the peculiarities of the application.</p>
<p>2.5.2。管理和战略规划。智能电网（SG）是一种电气系统，该系统以双向方式使用双向数字信息，网络安全通信技术和计算智能，跨越异构和分布式发电，输电，配电和消耗，以实现能源效率。 SG的一个关键和新颖的特征是智能层，它可以分析智能电表产生的数据，从而使公司能够在电网管理，规划和客户服务方面开发出强大的新功能，以提高能效。 SG的出现改变了能源的生产，定价和计费方式。 SG的关键方面是分布式能源生产，即可再生能源。可再生能源（太阳能，风能等）的渗透正在迅速增加，功率预测已成为确定要由输电系统运营商采用的运营计划政策的重要因素<br>        2.5.2. Management and Strategic Planning. The Smart Grid (SG) is an electric system that uses two-way digital information, cyber-secure communication technologies, and computational intelligence in an integrated fashion across heterogeneous and distributed electricity generation, transmission, distribution, and consumption to achieve energy efficiency. A key and novel characteristic of SGs is the intelligent layer that analyzes the data produced by smart meters, allowing companies to develop powerful new capabilities in terms of grid management, planning, and customer services for energy efficiency. The advent of SGs has changed the way energy is produced, priced, and billed. The key aspect of SGs is distributed energy production, namely , renewable energies. The penetration of renewable energies (solar , wind, etc.) is increasing quickly and power forecasting becomes an important factor in defining the operation planning policies to be adopted by a transmission system operator</p>
<p>在风电预测中观察文献时[Monteiro等。 2009]，人们意识到大多数建议都基于离线培训模式，建立了一个静态模型，然后用于生成预测。这种选择依赖于风力发电模型平稳性的假设，这一假设必须受到强烈质疑[Bremnes 2004;贝萨等。 2009]。使用来自三个不同风电场的真实数据，Bessa等人。 [2009]提出了在线训练相对于神经网络离线训练的优点。作者指出数据的不断发展的性质以及风模式行为中概念漂移的存在<br>        When observing the literature in wind power prediction [Monteiro et al. 2009], one realizes that most proposals are based on an of-line training mode, building a static model that is then used to produce predictions. This option relies on assumptions of stationarity of the wind electric power model, which must be strongly questioned [Bremnes 2004; Bessa et al. 2009]. Using real data from three distinct wind parks, Bessa et al. [2009] present the merits of online training against offline training of neural networks. The authors point out the evolving nature of data and the presence of concept drift in wind pattern behavior</p>
<p>2.5.3。个人协助和信息。数十年来，文本分类一直是机器学习中的热门话题。但是，与概念漂移问题有关的有趣应用相对较近。文本流应用程序的示例包括电子邮件分类[Carmona-Cejudo等。 2010]，电子邮件垃圾邮件检测[Lindstrom等。 2010]，以及情感分类[Bifet and Frank 2010]。情感分类是社交媒体监视，客户反馈分析和其他应用程序中的常见任务。<br>        2.5.3. Personal Assistance and Information. Text classification has been a popular topic in machine learning for decades. However , interesting applications related to the problem of concept drift appeared relatively recently . Examples of text stream applications include e-mail classification [Carmona-Cejudo et al. 2010], e-mail spam detection [Lindstrom et al. 2010], and sentiment classification [Bifet and Frank 2010]. Sentiment classification is a popular task in social media monitoring, customer feedback analysis, and other applications.</p>
<p><u><strong>(zhu: 感觉很像是GAN)</strong></u>电子邮件分类和垃圾邮件过滤中概念漂移的主要来源是由于<strong>电子邮件内容和表示方式的更改（虚拟漂移</strong>）以及<strong>垃圾邮件发送者</strong>试图<strong>克服</strong>垃圾邮件过滤器（可能是虚拟的或真实的）的<strong>自适应行为</strong>。此外，用户可能会<strong>改变</strong>对特定类别电子邮件的<strong>态度</strong>，开始或停止将其视为垃圾邮件（<strong>真正的漂移</strong>）。在情感分类中，用于<strong>表达积极情感和消极情感的词汇</strong>可能会随着时间而变化。由于文档集合不是静态的（虚拟漂移，新颖），因此表示当前集合的特征空间是动态的，这可能需要对模型进行特定的更新。<br>        The main source of concept drift in email classification and spam filtering are due to changing email content and presentation (virtual drift), as well as adaptive behavior of spammers trying to overcome spam filters (may be virtual or real). Besides, users may change their attitude toward particular categories of emails, starting or stopping to consider them spam (real drift). In sentiment classification, the vocabulary used to express positive and negative sentiments may change over time. Since the collection of documents is not static (virtual drift, novelties), the feature space representing the current collection is dynamic, which may require specific updates of the models.</p>
<p>在这一领域中已经使用了各种自适应学习策略，包括<strong>个体方法</strong>，例如基于<strong>案例的推理和集成</strong>，这些方法通过变化检测器不断发展或通过变化检测器进行显式检测（3.2节）。<br>        Various adaptive learning strategies have been used in this domain, including individual methods, like case-based reasoning, and ensembles, either evolving or with an explicit detection of changes by means of change detectors (Section 3.2).</p>
<p><strong>反馈的可用性</strong>是个人协助和信息方面的严峻挑战。<strong>难题是</strong>，如果<strong>反馈很容易获得</strong>，则<strong>意味</strong>着<strong>不需要</strong>自动预测。在电子邮件分类中，我们可以希望在<strong>分类错误</strong>的情况下不时<strong>收到用户的反馈</strong>，或者可以<strong>设计一个主动的学习系统</strong>（例如[Zliobaite et al。2014]），该系统会不时询问用户按需<strong>提供标签</strong>。但是，在可能的情况下，我们需要针对<strong>获取真实标签</strong>的<strong>自动</strong>方法。<br>        Availability of feedback is a serious challenge in personal assistance and information. The dilemma is that if feedback is easily available, that implies no need for automated predictions. In email classification, we can hope that from time to time we will receive feedback from the user in case of misclassifications or can design an active learning system (e.g., [Zliobaite et al. 2014]), which from time to time asks the user to provide labels on demand. However , when possible, we need to aim at automatic ways for obtaining the true labels.</p>
<p>假设要监视人们对政党的态度，我们想对<strong>Twitter推文</strong>的<strong>极性或情感</strong>进行分类。手动将推文标记为肯定或否定是一项艰巨且昂贵的任务。但是，推文可能具有作者提供的情绪指标：在使用各种类型的<strong>表情符号</strong>时，变化的<strong>情绪</strong>是<strong>隐含</strong>的。因此，我们可以使用它们来标记训练数据。笑脸或表情是与情绪状态相关的视觉提示。它们是使用标准键盘上可用的字符构造而成的，它们代表情感的面部表情。通过使用表情符号，推文的作者可以用情感状态注释自己的文本。带注释的推文可用于训练情感分类器。<br>        Suppose for monitoring the attitude of people toward a political party we want to classify the polarity or sentiment of tweets from Twitter . Labeling tweets manually as positive or negative is a laborious and expensive task. However , tweets may have author-provided sentiment indicators: changing sentiment is implicit in the use of various types of emoticons. Hence, we may use these to label the training data. Smileys or emoticons are visual cues that are associated with emotional states. They are constructed using the characters available on a standard keyboard, representing a facial expression of emotion. By using emoticons, authors of tweets annotate their own text with an emotional state. Annotated tweets can be used to train a sentiment classifier.</p>
<p>构建用于<strong>自适应新闻访问</strong>的基于内容的过滤器，对于流媒体设置中的文本分类提出了截然不同的观点。目的是逐步学习并保持最新的新闻报道分类用户模型。 Billsus和Pazzani [2000]提出了一种简单而有效的方法。对于<strong>每个用户</strong>，都构建了一个自适应学习系统，该系统由一个简单的集合和单独的模型组成，可满足用户的短期和长期利益。<strong>稳定</strong>的<strong>朴素贝叶斯分类器</strong>用于对用户的<strong>长期</strong>兴趣建模，最近的邻居分类器捕获用户的短期兴趣。对于<strong>短期</strong>兴趣模型，将在喜欢的新闻故事上保留<strong>一个固定大小的窗口</strong>，并且/或者根据其<strong>年龄</strong>对实例进行权衡。没有显式的<strong>更改检测</strong>用于<strong>监视</strong>短期或长期利益。由于<strong>积极的相关性反馈</strong>（例如，访问特定新闻项的用户提供该项与他或她的兴趣有关的信号），因此某些实例的<strong>真实标签</strong>自然而然地出现了。<br>        Building a content-based filter for adaptive news access presents a rather different perspective on text classification in streaming settings. The goal is to learn incrementally and keep up-to-date a user model for news story classification. A simple yet effective approach has been proposed in Billsus and Pazzani [2000]. For each user , an adaptive learning system is built, consisting of a simple ensemble with separate models for short-term and long-term interests of users. A stable naive Bayes classifier is used for modeling the long-term interests of a user and the nearest neighbor classifier captures the short-term interests of the user. For the short-term interests model, a fixed-size window over the liked news stories is maintained and/or instances are weighed with respect to their age. No explicit change detection is used for monitoring either of the short-term or long-term interests. The true labels of some of the instances come naturally due to a positive relevance feedback (i.e., a user accessing a particular news item provides the signal that the item is relevant to his or her interests)</p>
<p>另一方面，推荐系统在个人协助和信息类别中得到了广泛的应用[Bobadilla等。 2013; Adomavicius and Tuzhilin 2005]。 NetFlix竞赛提高了数据挖掘社区对推荐系统领域的兴趣。3获胜团队所汲取的教训之一是，将<strong>时间动态因素考虑在内</strong>对建立准确的模型至关重要。建模用户兴趣和处理概念漂移是其他有趣的方面。在协作过滤中，用户兴趣的建模主要取决于用户已经提供的其他评分的可用性。在实际的应用案例中，<strong>数据高度不平衡</strong>。有些电影很受欢迎，而大多数电影却不是。一些用户对许多电影进行评分，而其他一些用户则仅对其中一些进行评分。<strong>评级矩阵</strong>是<strong>高维</strong>且极为<strong>稀疏</strong>，仅包含约1％的非零元素。这样的特性使大多数监督学习技术的应用不适用，并刺激了<strong>高级协作过滤方法</strong>的发展。变化的来源和性质可以是多种多样的。项目和用户都在随着时间变化。物品的副作用首先包括改变<strong>产品的认知度和受欢迎度</strong>。预计某些电影的流行将遵循<strong>季节性模式</strong>。用户方面的影响包括<strong>用户品味</strong>和<strong>偏好</strong>的变化，其中一些可能是短期的或与环境有关的，因此可能会<strong>再次发生</strong>（情绪，活动，公司），评级规模的感知变化，家庭中评估者的可能变化以及类似的问题。获胜的团队开发了一种<strong>集成方法</strong>，包括用于处理这些各种变化的多种模型。正如Koren [2010]所建议的那样，流行的<strong>窗口化</strong>和<strong>实例权重方法</strong>并不是处理每种变化行为的最佳选择，这仅仅是因为在协作过滤中，例如，<strong>评级之间的关系对于预测性至关重要</strong>。造型。<br>        On the other hand, recommender systems are a broad application in the personal assistance and information category [Bobadilla et al. 2013; Adomavicius and Tuzhilin 2005]. Interests of the data mining community in the recommender systems domain have been boosted by the NetFlix competition.3One of the lessons learned by the winning teams was that taking temporal dynamics into account substantially contributes toward building accurate models. Modeling user interests and handling concept drift were the other interesting aspects. In collaborative filtering, modeling of user interests relies primarily on the availability of other ratings already provided by the users. In a realistic application case, the data is highly imbalanced. Some movies are very popular , while most of the movies are not; some users rate many movies, but many others rate only a few . The rating matrix is high dimensional and extremely sparse, containing only about 1% of nonzero elements. Such properties make the application of most supervised learning techniques inapplicable and motivate the development of advanced collaborative filtering approaches. The sources and the nature of change can be diverse. Both items and users are changing over time. The item-side effects include, first of all, changing product perception and popularity . Popularity of some movies is expected to follow seasonal patterns. The user-side effects include changing tastes and preferences of users, some of which may be short term or contextual and therefore likely reoccurring (mood, activity , company), changing perception of rating scale, possible change of rater within household, and similar problems. The winning team developed an ensemble approach including multiple models for handling these various kinds of changes. As suggested in Koren [2010], popular windowing and instance weighing approaches for handling concept drift are not the best choice for each kind of changing behavior , simply because in collaborative filtering, for example, the relations between ratings is of the main importance for predictive modeling.</p>
<p>2.5.4。<strong>无处不在的环境应用程序。</strong>自动驾驶汽车控制是无处不在的环境类别中的一个应用案例。 DARPA大挑战赛是为开发全自动无人驾驶汽车的团队开设的有奖竞赛。斯坦利（Stanley）是2005年举办的第二届长途比赛获胜团队的赛车，其车载系统高度可靠，可以使赛车以相对较高的速度穿越不同的越野环境。在系统的其他智能组件中，Stanley必须执行识别和避开障碍物和寻路的任务[Thrun等。 2006]。<br>        2.5.4. Ubiquitous Environment Applications. Autonomous vehicle control is an application case in the ubiquitous environment category . DARPA Grand Challenge was a prize competition open for teams developing fully autonomous driverless vehicles. Stanley , the car of the winning team of the second long-distance competition4organized in 2005, had a highly reliable system on board that allowed the car to drive through different off-road environments at relatively high speeds. Among other intelligent components of the systems, Stanley had to perform identification and avoidance of obstacles and road finding [Thrun et al. 2006].</p>
<p>Stanley团队结合了一些自动地形标记的想法，使用<strong>流</strong>激光传感器数据来避开中短程障碍，并使用彩色相机的流数据来学习可驱动表面的概念并将其用于速度控制（如果表面进一步前方是不可行驶的，汽车应减速）。也就是说，视觉模块必须维护一个<strong>准确的分类器</strong>，以识别图像流中的可驱动区域和不可驱动区域。<strong>自适应学习方法</strong>对于可靠地执行此任务是必要的，因为会影响目标概念的许多<strong>变化且不易测量</strong>的因素（例如表面材料，光照条件或照相机本身上的灰尘或污垢）。<br>        The Stanley team combined several ideas for automated terrain labeling using streaming laser sensor data for short- and medium-range obstacle avoidance and streaming data from the color camera to learn the concept of the drivable surface and use it for velocity control (if the surface further ahead is nondrivable the car should slow down). That is, the vision module had to maintain an accurate classifier for identifying drivable and nondrivable regions in the image stream. An adaptive learning approach was necessary for performing this task reliably because of many changing and not easily measurable factors such as surface material, lighting conditions, or dust or dirt on the camera itself that affect the target concept.</p>
<p>分类任务是为可驾驶地形的颜色建模。斯坦利小组使用了高斯人的自适应混合。该模型必须适应缓慢变化的照明条件和突然变化的表面颜色（例如，从铺砌的道路变为未铺砌的道路）。为了逐步适应，对内部高斯进行了调整。为了快速适应，高斯人被新人取代。所需的适应速度取决于路况。<br>        The classification task was to model the color of the drivable terrain. The Stanley team used an adaptive mixture of Gaussians. The model had to adapt to slowly changing lighting conditions and to abruptly changing surface colors (e.g., moving from a paved to an unpaved road). For gradual adaptation, the internal Gaussian was adjusted; for the rapid adaptation, the Gaussians were replaced with new ones. The required speed of adaptation depended on the road conditions.</p>
<p>这些实际应用案例提供了证据，表明自适应学习可以解决在固定环境下无法解决的复杂学习问题。这些示例还说明，首先重要的是要<strong>了解漂移的性质和来源</strong>，然后才能设计出有效的自适应学习策略<br>        These real-world application cases present evidence that adaptive learning makes it possible to address complex learning problems that would not be feasible to tackle in the stationary settings. These examples also illustrate that it is important first to understand the nature and source of drift and only then engineer an effective adaptive learning strategy</p>
<h2 id="3-概念漂移适应方法的分类法"><a href="#3-概念漂移适应方法的分类法" class="headerlink" title="3.概念漂移适应方法的分类法"></a>3.概念漂移适应方法的分类法</h2><ol start="3">
<li>TAXONOMY OF METHODS FOR CONCEPT DRIFT ADAPTATION</li>
</ol>
<p>在本节中，我们为自适应算法提出了一种新的<strong>分类法</strong>，该算法从具有<strong>未知动态</strong>的不断发展的数据中学习了预测模型。两个主要的抽象概念是<strong>内存</strong>和<strong>遗忘数据和/或模型</strong>。我们讨论了一组实现自适应策略的代表性算法和流行算法。讨论围绕自适应学习算法的四个<strong>模块</strong>进行组织，这些模块在图3中进行了标识：内存，更改检测，学习和无损估计。在四个单独的模块而不是<strong>方法树中呈现分类法的背后的主要思想</strong>是，将自适应学习系统视为由<strong>模块化</strong>组件组成的模块组件，这些组件可以进行<strong>排列</strong>和相互<strong>组合</strong>。图4概述了该分类法，图5、6、7和8进一步详细介绍了每个模块中的方法。<br>        In this section, we propose a new taxonomy for adaptive algorithms that learn a predictive model from evolving data with unknown dynamics. The two main abstract concepts are memory and forgetting data and/or models. We discuss a set of representative and popular algorithms that implement adaptation strategies. The discussion is organized around four modules of adaptive learning algorithms that were identified in Figure 3: memory, change detection, learning, a n d loss estimation. The main idea behind presenting the taxonomy in four separate modules rather than a tree of methods is to see adaptive learning systems as consisting of modular components, which can be permuted and combined with each other . An overview of the taxonomy is presented in Figure 4, and the methods within each module are further detailed in Figures 5, 6, 7, and 8.</p>
<p>3.1。记忆<br>        3.1. Memory</p>
<p>在概念漂移下学习不仅需要<strong>用新信息更新预测模型</strong>，而且还需要<strong>忘记旧信息</strong>。我们考虑两种类型的存储器：表示为<strong>数据</strong>的<strong>短期</strong>存储器和表示为<strong>数据模型的概括</strong>的<strong>长期</strong>存储器。在本小节中，我们将在如图5所示的两个维度下<strong>分析短期</strong>记忆：（i）<strong>数据管理</strong>指定用于学习的数据，（ii）<strong>遗忘机制</strong>指定丢弃旧数据的方式。<strong>长期</strong>记忆的问题将在第<strong>3.3</strong>节中讨论。<br>        Learning under concept drift requires not only updating the predictive model with new information but also forgetting the old information. We consider two types of memory: short-term memory represented as data and long-term memory represented as generalization of data models. In this subsection, we analyze the short-term memory under two dimensions as illustrated in Figure 5: (i) data management specifies which data is used for learning, and (ii) forgetting mechanism specifies in which way old data is discarded. The issues of long-term memory will be discussed in Section 3.3.</p>
<p>3.1.1。<strong>数据管理</strong>。大多数自适应学习算法背后的<strong>假设</strong>是，<strong>最新数据</strong>对于当前预测而言是最有用的。因此，数据管理通常旨在<strong>从最新数据中学习</strong>：单个示例或多个示例。<br>        3.1.1. Data Management. The assumption behind the most of adaptive learning algorithms is that the most recent data is the most informative for the current prediction. Hence, data management typically aims at learning from the most recent data: either a single example or multiple examples.</p>
<p><strong>单个实例</strong>。仅将一个示例存储在内存中是<strong>源于</strong>在线学习算法，该算法<strong>一次只能从一个示例</strong>中学习，并且以后无法访问前面的示例。在线算法可以<strong>按到达顺序依次处理其输入示例</strong>，而无需将整个训练数据集保存在内存中。大多数在线学习者<strong>维护一个假设</strong>（该假设可能源自复杂的预测模型），并且<strong>模型更新受错误驱动</strong>。当一个新的例子Xtarriv出现时，由当前假设做出的预测ytis。当收到真实目标值ytis时，将计算损失，并在必要时更新当前假设。这种算法的一个例子是WINNOW [Littlestone 1987]，它是使用<strong>乘法加权更新</strong>方案的<strong>线性分类器</strong>系统。 <strong>WINNOW</strong>的关键特征是其<strong>对无关功能的鲁棒性</strong>。<br>        Single Example. Storing only a single example in memory has roots in online learning algorithms that learn from one example at a time and do not have access to the previous examples later . An online algorithm can process its input example by example in the order of arrival without keeping the entire training dataset in memory . Most online learners maintain a single hypothesis (which can originate from a complex predictive model) and model updates are error driven. When a new example Xtarrives, a prediction ˆ ytis made by the current hypothesis. When the true target value ytis received, loss is computed and the current hypothesis is updated if necessary . An example of such algorithms is WINNOW [Littlestone 1987], which is a linear classifier system that uses a multiplicative weight-update scheme. The key characteristic of WINNOW is its robustness to irrelevant features.</p>
<p>在线学习算法可以<strong>自然地适应不断变化</strong>的分布，这主要是由于在线学习算法的<strong>学习机制</strong>可以<strong>使用最新示例不断更新模型</strong>。但是，在线学习系统没有<strong>明确的遗忘机制</strong>。仅当旧概念由于新的传入数据而被<strong>淡化时</strong>，<strong>才会发生适应</strong>。诸如WINNOW [Littlestone 1987]和VFDT [Domingos and Hulten 2000]之类的系统<strong>可以适应</strong>随时间变化的<strong>缓慢变化</strong>。主要限制是它们对<strong>突变的适应性较慢</strong>，这取决于设置<strong>新示例</strong>的模型<strong>更新的敏感性</strong>。设置这些参数需要在<strong>稳定性</strong>和<strong>灵敏度</strong>之间进行<strong>权衡</strong>[Carpenter等。 1991b]。明确处理概念漂移的代表性单实例内存系统包括STAGGER [Schlimmer and Granger 1986]，DWM [Kolter and Maloof 2003，2007]，SVM [Syed等。 1999]，<strong>IFCS</strong>[Bouchachia 2011a]和GT2FC [Bouchachia and Vanaret 2013]。<br>        Online learning algorithms can be seen as naturally adaptive to evolving distributions, mainly due to their learning mechanisms that continuously update the model with the most recent examples. However , online learning systems do not have explicit forgetting mechanisms. Adaptation happens only as the old concepts are diluted due to the new incoming data. Systems like WINNOW [Littlestone 1987] and VFDT [Domingos and Hulten 2000] can adapt to slow changes over time. The main limitation is their slow adaptation to abrupt changes, which depends on how sensible the model update with a new example is set to be. Setting these parameters requires a tradeoff between stability and sensitivity [Carpenter et al. 1991b]. Representative single instance memory systems that explicitly deal with concept drift include STAGGER [Schlimmer and Granger 1986], DWM [Kolter and Maloof 2003, 2007], SVM [Syed et al. 1999], IFCS [Bouchachia 2011a], and GT2FC [Bouchachia and Vanaret 2013].</p>
<p>多个例子。数据管理的另一种方法是<strong>保持与一组最新示例一致的预测模型</strong>。算法族FLORA [Widmer and Kubat 1996]是最早的有监督的用于数据演化的增量学习系统之一。原始的FLORA算法使用<strong>固定长度</strong>的<strong>滑动窗口</strong>，该窗口将最新的示例存储在<strong>先进先出（FIFO）</strong>数据结构中。在每个时间步上，学习算法都会使用训练窗口中的示例构建新模型。通过以下两个过程来更新模型：学习过程（基于新数据更新模型）和遗忘过程（丢弃从窗口移出的数据）。关键的挑战是选择<strong>合适的窗口尺寸</strong>。<strong>较短</strong>的窗口可以更准确地反映电流分布；因此，它可以确保在概念变化时<strong>快速适应</strong>，但是在<strong>稳定</strong>时期，窗口太短会降低系统性能。<strong>大</strong>窗口可在<strong>稳定的时间</strong>内提供<strong>更好的性能</strong>，但对概念变化的反应速度较慢<br>        Multiple Examples. Another approach to data management is to maintain a predictive model consistent with a set of recent examples. The algorithm family FLORA [Widmer and Kubat 1996] is one of the first supervised incremental learning systems for evolving data. The original FLORA algorithm uses a sliding window of a fixed length, which stores the most recent examples in the first-in-first-out (FIFO) data structure. At each time step, the learning algorithm builds a new model using the examples from the training window . The model is updated following two processes: a learning process (update the model based on the new data) and a forgetting process (discard data that is moving out of the window). The key challenge is to select an appropriate window size. A short window reflects the current distribution more accurately; thus, it can ensure fast adaptation in times with concept changes, but during stable periods, a too short window worsens the performance of the system. A large window gives a better performance in stable periods, but it reacts to concept changes more slowly</p>
<p>通常，训练窗口的大小可以是固定的，也可以随时间变化。<br>        In general, the training window size can be fixed or variable over time.</p>
<p>固定大小的推拉窗。在内存中存储固定数量的最新示例。每当出现新示例时，它将被保存到内存中，而最旧的示例将被丢弃。这种<strong>简单</strong>的自适应学习方法通常用作<strong>评估新算法</strong>的<strong>基准</strong>。</p>
<p>大小可变的滑动窗口。通常<strong>根据时间变化检测器</strong>的<strong>指示</strong>，随时间<strong>变化</strong>窗口中示例的数量。一种简单的方法是在每<strong>次更改被选中时缩小</strong>窗口，以使训练数据反映最新的概念，<strong>否则增大</strong>窗口。<br>        Sliding windows of a fixed size. Store in memory a fixed number of the most recent examples. Whenever a new example arrives, it is saved to memory and the oldest one is discarded. This simple adaptive learning method is often used as a baseline in evaluation of new algorithms. Sliding windows of variable size. Vary the number of examples in a window over time, typically depending on the indications of a change detector . A straightforward approach is to shrink the window whenever a change is singled such that the training data reflects the most recent concept and grow the window otherwise.</p>
<p>使用自适应窗口大小的<strong>最早算法</strong>之一是FLORA2 [Widmer and Kubat 1996]。传入的示例已添加到窗口，最旧的示例已删除。<strong>添加和删除</strong>使窗口（和预测模型）与当前概念保持一致。该算法的其他版本处理<strong>重复概念</strong>（FLORA3）和<strong>嘈杂数据</strong>（FLORA4）。后来的研究[Klinkenberg and Joachims 2000]提出了一种理论上支持的方法，该方法用于<strong>识别和处理</strong>支持向量机（SVM）的概念变化。此方法在训练示例上保持适当大小的窗口。关键思想是根据<strong>泛化误差的估计</strong>来<strong>调整窗口大小</strong>。在每个时间步，该算法<strong>使用各种窗口大小构建许多SVM</strong>模型，并选择将<strong>遗忘一错误估计最小化</strong>的窗口大小作为训练窗口<br>        One of the first algorithms using an adaptive window size was the FLORA2 [Widmer and Kubat 1996]. Incoming examples are added to the window and the oldest ones are deleted. Addition and deletion keep the window (and the predictive model) consistent with the current concept. Further versions of the algorithm deal with recurring concepts (FLORA3) and noisy data (FLORA4). A later study [Klinkenberg and Joachims 2000] presents a theoretically supported method for recognizing and handling concept changes with support vector machines (SVMs). This method maintains a window over the training examples with an appropriate size. The key idea is to adjust the window size based on the estimate of the generalization error . At each time step, the algorithm builds a number of SVM models using various window sizes and selects the the window size that minimizes the leave-one-out error estimate as the training window</p>
<p>可变长度的学习窗口出现在Maloof和Michalski [1995]，Klinkenberg [2004]，Gama等人中。 [2004]，以及Kuncheva和Zliobaite [2009]。依赖于窗口背后的假设是<strong>数据的新近度与相关性和重要性相关联。</strong>不幸的是，这种假设可能在每种情况下都不正确。例如，当<strong>数据嘈杂或概念再次出现时，数据的新近并不意味着相关</strong>。而且，如果<strong>慢速变化持续的时间超过窗口大小</strong>，则窗口化也会失败。<br>        Learning windows of variable length appear in Maloof and Michalski [1995], Klinkenberg [2004], Gama et al. [2004], and Kuncheva and Zliobaite [2009]. The assumption behind relying on windowing is that the recency of the data is associated with relevance and importance. Unfortunately , this assumption may not be true in every circumstance. For instance, when the data is noisy or concepts reoccur , recency of data does not mean relevance. Moreover , if slow change lasts longer than the window size, windowing may fail as well.</p>
<p>3.1.2。<strong>忘记机制</strong>。处理由动态未知的过程生成的不断发展的数据的最常见方法是<strong>忘记过时的观察结果</strong>。遗忘机制的选择取决于我们对<strong>数据分布变化的期望</strong>，以及在<strong>反应性和对系统噪声的鲁棒性</strong>之间的权衡。遗忘越突然，反应性就越快，但是<strong>捕获噪音的风险</strong>也就越高。<br>        3.1.2. Forgetting Mechanism. The most common approach to deal with evolving data generated from processes with unknown dynamics is forgetting the outdated observations. The choice of a forgetting mechanism depends on the expectations that we have regarding changes in data distribution and what tradeoff between reactivity and robustness to noise of the system is required. The more abrupt forgetting is, the faster the reactivity , but also the higher the risk is of capturing noise.</p>
<p>突然忘记。每次都有一组观察结果定义了考虑学习的信息窗口。<strong>突然遗忘或部分记忆是</strong>指给定观察结果在训练窗口之内或之外的机制。文献中已经提出了几种窗口模型。滑动窗的两种基本类型是[Babcock等。 [2002]（i）<strong>基于序列</strong>，其中窗口的大小由观察数表征，以及（ii）<strong>基于时间戳</strong>，其中窗口的大小由持续时间定义。<br>        Abrupt Forgetting. At each time, a set of observations defines a window of the information considered for learning. Abrupt forgetting, or partial memory , refers to the mechanisms where a given observation is either inside or outside the training window . Several window models have been presented in the literature. Two basic types of sliding windows are [Babcock et al. 2002] (i) sequence based, where the size of a window is characterized by the number of observations, and (ii) timestamp based, where the size of a window is defined by duration time.</p>
<p>基于序列的窗口有两种模型：大小为j的<strong>滑动窗口</strong>和<strong>界标窗口。</strong>滑动窗口仅在<strong>先进先出（</strong>FIFO）数据结构中存储最新的示例，而<strong>地标窗口</strong>在给定的timsetamp（称为地标）之后存储所有示例。<strong>界标窗口是可变大小的窗口</strong>。大小为<strong>t的基于时间戳的窗口</strong>包括从t单位时间到现在的时间内到达的所有元素。上一节讨论了使用突然遗忘的学习系统。<br>        There are two models for sequence-based windows: sliding windows of size j and landmark windows. A sliding window stores only the most recent examples in the firstin-first-out (FIFO) data structure, while a landmark window stores all the examples after a given timsetamp, which is called a landmark. A landmark window is a window of variable size. A timestamp-based window of size t includes all the elements that arrived within the time from t units of time back until now . Learning systems that use abrupt forgetting were discussed in the previous section.</p>
<p><strong>(zhu 这个好像怪怪的, 这个reservoir 采样)</strong> <strong>克服窗口化</strong>（尤其是固定窗口化）的替代方法之一是<strong>采样。</strong>目的是<strong>总结长时间内数据流的基本特征</strong>，以便从数据流中<strong>均匀地提取</strong>每个包含的样本。一种众所周知的算法是<strong>储层采样</strong>[Vitter 1985]。储层采样的目的是为观测到的水流获得<strong>代表性的样本</strong>。操作如下。当ithitem到达时，它以p = k / i的概率存储在容器中，其中<strong>k是容器的大小</strong>，并以<strong>1-p的概率丢弃</strong>。如果为正，则将<strong>随机选择的样本从储罐中丢弃，以释放新样本的空间</strong>。已经研究了许多采样技术，例如Al-Kateb等。 [2007]，Efraimidis和Spirakis [2006]，Aggarwal [2006]和Rusu和Dobra [2009]。 Y ao等人在与漂移和变化检测有关的一些研究中讨论了储层采样，并作为Ng和Dash [2008]中窗口化的替代方法。 [2012]，Zhao等。 [2011]。<br>        One of the alternatives to overcome windowing, especially fixed windowing, is sampling. The goal is to summarize the underlying characteristics of a data stream over long periods of time such that every included sample is drawn uniformly from the stream. One well-known algorithm is the reservoir sampling [Vitter 1985]. The goal of the reservoir sampling is to obtain a representative sample for the observed stream. It operates as follows. When the ithitem arrives, it is stored in the reservoir with the probability p = k/i, where k is the size of the reservoir , and discarded with the probability 1 − p. If positive, then a randomly chosen sample is discarded from the reservoir to free space for the new sample. A number of sampling techniques have been investigated, like in Al-Kateb et al. [2007], Efraimidis and Spirakis [2006], Aggarwal [2006], and Rusu and Dobra [2009]. Reservoir sampling has been discussed in some studies related to drift and change detection and as an alternative to windowing in Ng and Dash [2008], Y ao et al. [2012], and Zhao et al. [2011].</p>
<p><strong>逐渐忘记</strong>。逐渐遗忘是一种<strong>完整的内存方法</strong>，这意味着不会从内存中完全丢弃任何示例。内存中的<strong>示例与反映其年龄的权重相关联</strong>。示例权重基于一个简单的直觉，即示例在培训集中的重要性应随年龄的增长而降低。假设在时间i，存储的足够统计量为Si-1，我们观察一个示例Xi。假设有一个聚合函数G（X，S），则将新的足够统计量计算为$S_i = G（X_i，αS_{i-1}）$，其中α∈（0,1）是衰落因子；这样，最早的信息就变得最不重要。<br>        Gradual Forgetting. Gradual forgetting is a full memory approach, which means that no examples are completely discarded from the memory . Examples in memory are associated with weights that reflect their age. Example weighting is based on a simple intuition that the importance of an example in the training set should decrease with its age. Suppose that at time i, the stored sufficient statistics is Si−1and we observe an example Xi. Assuming an aggregation function G(X, S), the new sufficient statistics are computed as Si= G(Xi,αSi−1), where α ∈ (0,1) is the fading factor; this way the oldest information becomes the least important.</p>
<p>线性衰减技术可以在Koychev [2000，2002]中找到，而<strong>指数衰减技术</strong>可以在Klinkenberg [2004]中找到。后一种技术使用<strong>指数老化</strong>函数$w_λ（X）= exp（-λj）$根据示例的年龄对示例进行加权，其中示例X出现在j步之前。参数λ控制权重降低的速度。对于较大的λ值，将较低的权重分配给旧示例，因为它们被认为对当前预测的信息较少。如果λ= 0，则所有示例的权重相同。<br>        Linear decay techniques can be found in Koychev [2000, 2002], and a technique for exponential decay is presented in Klinkenberg [2004]. The latter technique weights examples according to their age using an exponential aging function wλ(X) = exp(−λj), where the example Xappeared j time steps ago. The parameter λ controls how fast the weights decrease. For larger values of λ, a lower weight is assigned to old examples, as they are deemed to be less informative for the current prediction. If λ = 0, all the examples have the same weight.</p>
<h3 id="3-2。变更检测"><a href="#3-2。变更检测" class="headerlink" title="3.2。变更检测"></a>3.2。变更检测</h3><p>3.2. Change Detection</p>
<p>变化检测组件是指用于<strong>显式漂移</strong>和<strong>变化检测</strong>的技术和机制。它通过<strong>识别变化点</strong>或<strong>变化发生的小时间间隔</strong>来表征和量化概念漂移[Basseville and Nikiforov 1993]。我们考虑如图6所示的以下维度：（i）基于<strong>顺序分析</strong>的方法，（ii）基于<strong>控制图</strong>的方法，（iii）基于两种<strong>分布之间的差异</strong>的方法，以及（iv）<strong>启发式方法</strong>。</p>
<p>我们已经指出，在线学习系统无需任何明确的变更检测机制，就可以适应不断发展的数据。显式更改检测的<strong>优点</strong>是<strong>提供有关过程生成数据动态</strong>的信息。<br>        The change detection component refers to the techniques and mechanisms for explicit drift and change detection. It characterizes and quantifies concept drift by identifying change points or small time intervals during which changes occur [Basseville and Nikiforov 1993]. We consider the following dimensions as illustrated in Figure 6: (i) methods based on sequential analysis, (ii) methods based on control charts, (iii) methods based on differences between two distributions, and (iv) heuristic methods. We already pointed out that online learning systems, without any explicit change detection mechanism, can adapt to evolving data. The advantage of explicit change detection is providing information about the dynamics of the process generating data.</p>
<p>一个典型的部门策略监视绩效<strong>指标的演变</strong>[Widmer and Kubat 1996; Zeira等。 [2004年]<strong>或原始数据</strong>，然后将它们<strong>与固定的基线进行统计比较</strong>。具有<strong>监控性能指标</strong>的自适应学习的一项<strong>开创性</strong>工作是FLORA系列算法。 FLORA2算法[Widmer and Kubat 1996]包括基于规则的分类器的<strong>窗口调整试探法</strong>。为了检测概念变化，将<strong>监视当前模型的准确性和覆盖范围</strong>，并<strong>相应地调整窗口大小</strong>。在信息过滤的背景下，已经提出了<strong>监视三个性能指标（准确性，召回率和精确度</strong>）的值的建议[Klinkenberg and Renz 1998]。将<strong>每个指标的后验与移动平均值的标准样本误差进行比较</strong>。<br>        A typical section strategy monitors the evolution of the performance indicators [Widmer and Kubat 1996; Zeira et al. 2004] or raw data and statistically compares them to a fixed baseline. A seminal work in adaptive learning with monitoring performance indicators is the FLORA family of algorithms. The FLORA2 algorithm [Widmer and Kubat 1996] includes a window adjustment heuristic for a rule-based classifier. To detect concept changes, the accuracy and the coverage of the current model are monitored and the window size is adapted accordingly . In the context of information filtering, monitoring the values of three performance indicators—accuracy, recall, and precision—has been proposed [Klinkenberg and Renz 1998]. The posterior of each indicator is compared to the standard sample errors of a moving average value.</p>
<p>3.2.1。基于<strong>顺序分析</strong>的检测器<strong>。顺序概率比率测试</strong>（SPRT）[Wald 1947]是几种变更检测算法的基础。令$X_{1}^{n}$为示例序列，其中示例X的子集从未知分布$P_0$生成，子集$X_{1}^{w}$从另一个未知分布$P_1$生成。当基础分布在点w从P0变为P1时，在P1下观察到某些子序列的可能性预计将大大高于P0下。Significance 是两个概率之比不小于阈值。假设观测值$X_i$是独立的，则用于检验在时间w发生变化点的假设与在时间w不变的零假设的检验的统计量由</p>
<p>$T_{w}^{n}=\log \frac{P\left(x_{w} \ldots x_{n} \mid P 1\right)}{P\left(x_{w} \ldots x_{n} \mid P 0\right)}=\sum_{i=w}^{n} \log \frac{P 1\left[x_{i}\right]}{P 0\left[x_{i}\right]}=T_{w}^{n-1}+\log \frac{P 1\left[x_{n}\right]}{P 0\left[x_{n}\right]}$<br>if $T_{w}^{n}&gt;L$则发出变化信号，其中L是用户定义的阈值。</p>
<p>3.2.1. Detectors Based on Sequential Analysis. The Sequential Probability Ratio Test (SPRT) [Wald 1947] is the basis for several change detection algorithms. Let Xn 1be a sequence of examples, where the subset of examples X generated from an unknown distribution P0, and the subset Xn wis generated from another unknown distribution P1. When the underlying distribution changes from P0to P1at point w, t h e probability of observing certain subsequences under P1is expected to be significantly higher than that under P0. Significance means that the ratio of the two probabilities is not smaller than a threshold. Assuming that observations Xiare independent, the statistic for testing the hypothesis that a change point occurred at time w against the null hypothesis that there was no change at time w is given by<br>        and a change is signaled if Tn w&gt; L, where L is a user-defined threshold.</p>
<p>由于Page [1954]使用SPRT原理，所以累积和（CUSUM）是一种顺序分析技术。它通常用于<strong>变更检测</strong>。当输入数据的<strong>平均值显着偏离零时</strong>，测试将输出警报。测试的<strong>输入是任何预测变量的残差</strong>，例如，卡尔曼滤波器的预测误差。 CUSUM测试由gt = max（0，gt-1 +（xt-δ））（g0 = 0）给出，并且<strong>决定规则是</strong>，如果gt&gt;<strong>λ则发出警报</strong>，<strong>然后设置gt = 0</strong>。代表当前观测值，δ表示允许的变化量，t是当前时间，λ是用户定义的阈值。由于此规则仅检测正向的变化，因此当需要发现负向变化时，应使用min而不是max。在这种情况下，当gtis小于（负）阈值λ时，<strong>发出变化信号</strong>。 CUSUM测试是<strong>无记忆的</strong>，其准确性取决于<strong>参数δ和λ</strong>的选择。这两个参数控制着在<strong>较早检测到真实变化与允许更多错误警报之间的权衡</strong>。较低的δ值可以加快检测速度，但会增加误报次数。例如，Muthukrishnan等人将CUSUM应用于流采矿。 [2007]。<br>        The cumulative sum (CUSUM) is a sequential analysis technique due to Page [1954] that uses principles of SPRT . It is often used for change detection. The test outputs an alarm when the mean of the incoming data significantly deviates from zero. The input for the test is a residual from any predictor , for instance, the prediction error from a Kalman filter. The CUSUM test is given by gt= max(0,gt−1+ (xt− δ)) (g0= 0), and the decision rule is if gt&gt; λ then signal an alarm followed by setting gt= 0. Here xt stands for the current observed value, δ corresponds to the magnitude of changes that are allowed, t is the current time, and λ is a user-defined threshold. Since this rule only detects changes in the positive direction, when negative changes need to be found, min should be used instead of max. In this case, a change is signaled when gtis less than a (negative) threshold λ. The CUSUM test is memoryless, and its accuracy depends on the choice of parameters δ and λ. Both parameters control the tradeoff between earlier detecting the true changes and allowing more false alarms. Low values of δ allow faster detection, at a cost of increased number of false alarms. CUSUM has been applied in stream mining, for example, by Muthukrishnan et al. [2007].</p>
<p>Page-Hinkley [Page 1954]测试（PH）是CUSUM的一种变体。它是一种顺序分析技术，通常用于信号处理中的变化检测。它允许有效地检测由模型建立的过程的正常行为的变化。 PH测试是对高斯信号的平均值突然变化的检测的顺序适应[Mouss等。 2004]。测试变量mTis定义为直到当前时间为止的观测值与它们的平均值之间的累积差：mT =？T t = 1（xt-？xT-δ），其中？xT = 1 T？T t = 1xt和δ指定可容忍的变化幅度。最小mTis定义为MT = min（mt，t = 1 … T）。 PH测试MT和mT之间的差异：PHT = mT- MT。当此差异大于阈值（λ）（用户定义）时，将标记更改。较大的λ会导致较少的误报，但可能会错过一些更改。<br>        The Page-Hinkley [Page 1954] test (PH) is a variant of CUSUM. It is a sequential analysis technique typically used for change detection in signal processing. It allows efficient detection of changes in the normal behavior of a process established by a model. The PH test is a sequential adaptation of the detection of an abrupt change in the average of a Gaussian signal [Mouss et al. 2004]. The test variable mTis defined as the cumulative difference between the observed values and their mean up until the current time: mT=?T t=1(xt− ¯ xT−δ), where ¯ xT= 1 T ?T t=1xtand δ specifies the tolerable magnitude of changes. The minimum mTis defined as MT= min(mt,t = 1…T). PH tests for the difference between MTand mT: PHT= mT− MT. When this difference is greater than a threshold (λ) (user defined), a change is flagged. Larger λ will entail fewer false alarms but might miss some changes.</p>
<p>与在线版本的CUSUM和PH测试类似，Shiryae的贝叶斯测试[Shiryaev 2009; Tartakovsky和Moustakides 2010]依靠<strong>在线阈值确定</strong>，即，一旦计算出的<strong>统计量超过预设阈值ε</strong>，就可以诊断出这种变化。这种检测方法的<strong>准确性通常取决于误报率和误检测率</strong>。以下小节将进一步说明其中一些方法。<br>        Similar to the online versions of CUSUM and PH tests, Shiryae’s Bayesian test [Shiryaev 2009; Tartakovsky and Moustakides 2010] rely on online thresholding, that is, as soon as the computed statistic exceeds a preset threshold ε, the change is diagnosed. The accuracy of such detection methods often depends on the false alarm rate and the misdetection rate. Some of these methods are further explained in the following subsections.</p>
<p>3.2.2。基于<strong>统计过程控制的</strong>检测器。控制图或统计过程控制（SPC）是用于在连续制造过程中监视和控制产品质量的标准统计技术。 SPC将学习视为一个过程，并监视该过程的发展。基于<strong>SPC</strong>的漂移检测方法出现在Klinkenberg和Renz [1998]，Lanquillon [2002]，Gama等人中。 [2004]，Gomes等。 [2011]和Bouchachia [2011a]。<br>        3.2.2. Detectors Based on Statistical Process Control. Control charts, or statistical process control (SPC), are standard statistical techniques to monitor and control the quality of a product during a continuous manufacturing. SPC considers learning as a process and monitors the evolution of this process. Drift detection methods based on SPC appear in Klinkenberg and Renz [1998], Lanquillon [2002], Gama et al. [2004], Gomes et al. [2011], and Bouchachia [2011a].</p>
<p>令对（Xi，yi）构成示例序列。对于每个示例，模型都会预测ˆ yi，它可以为true（ˆ yi = yi）或为假（ˆ yi？= yi）。对于一组示例，该误差是来自<strong>伯努利试验的随机变量</strong>。<strong>二项式分布</strong>给出了随机变量概率的一般形式，该概率表示一组nexamples中的错误数。对于序列中的每个点i，<strong>错误率</strong>是观察到概率为pi的概率pi，其标准偏差为σi=？pi（1- pi）/ i。漂移检测器在模型操作期间<strong>管理两个寄存器</strong>pmi n和σminA tt im ei，在对当前示例进行预测并验证预测误差之后，如果pi +σiis低于pmi n +σmin，则pmi n = pi和σmin = σi。<br>        Let pairs (Xi, yi) form a sequence of examples. For each example, the model predicts ˆ yi, which can be either true ( ˆ yi= yi) o r false ( ˆ yi?= yi). For a set of examples, the error is a random variable from Bernoulli trials. The binomial distribution gives a general form of the probability for the random variable that represents the number of errors in a set of nexamples. For each point i in the sequence, the error rate is the probability pi of observing false with the standard deviation σi= ?pi(1 − pi)/i. The drift detector manages two registers during the model operation, pmi nandσmi n.A tt i m ei,after casting the prediction for the current example and verifying the prediction error , if pi+ σiis lower than pmi n+ σmi n, t h e n pmi n= piand σmi n= σi.</p>
<p>对于<strong>足够多的观测值</strong>，二项式分布通过<strong>均值和方差相同的正态分布</strong>进行近似。考虑到除非概念漂移发生，否则<strong>概率分布不应改变</strong>，因此对于<strong>pi&gt; n&gt; 30</strong>的示例，1-δ/ 2置信区间约为pi±α×σi。参数α取决于期望的置信度。阈值pi +σi≥pmi n + 2 ∗σmin时，<strong>警告</strong>的常用置信度为95％，阈值pi +σi≥pmi n + 3 ∗σmin时，<strong>失控率</strong>为99％。<br>        For a sufficiently large number of observations, the binomial distribution is closely approximated by the normal distribution with the same mean and variance. Considering that the probability distribution should not change unless a concept drift happens, the 1−δ/2 confidence interval for piwith n &gt; 30 examples is approximately pi±α×σi. The parameter α depends on the desired confidence level. A commonly used confidence level for Warning is 95% with the threshold pi+ σi≥ pmi n+ 2 ∗ σmi n, a n d f o r Out-of-Control is 99% with the threshold pi+ σi≥ pmi n+ 3 ∗ σmi n.</p>
<p>假设在时间j出现一个示例（Xj，yj），并且模型预测得出pj和σj。该系统定义为以下三种状态之一：</p>
<p>（1）In-Control，而pj +σjpmi n + 2×σmin。系统错误稳定。示例Xji被认为与前面的示例来自相同的分布。 </p>
<p>（2）每当pj +σj≥pmi n + 3×σmin时失控。与最近的过去示例相比，该错误已大大增加。最近的例子有99％的概率来自与前面的例子不同的分布。 </p>
<p>（3）<strong>警告状态</strong>介于前两个状态之间。错误正在增加，但尚未达到失控状态。这不是决定性的状态。误差可能由于<strong>噪声，漂移或预测模型的小缺陷而增加</strong>。此状态表示需要更多示例来确认漂移。<br>        Suppose at time j an example (Xj, yj) arrives and the model prediction leads to pj and σj. The system is defined to be in one of the following three states: (1) In-Control while pj+ σj pmi n+ 2 × σmi n. The error of the system is stable. The example Xjis deemed to come from the same distribution as the previous examples. (2) Out-of-Control whenever pj+ σj≥ pmi n+ 3 × σmi n. The error has increased significantly as compared to the recent past examples. With a probability of 99%, the recent examples come from a different distribution than the previous examples. (3) Warning state is in between the two previous states. The error is increasing but has not reached Out-of-Control yet. This is a not a decisive state. The error may be increasing due to noise, drift, or a small deficiency of the predictive model. This state signals that more examples are required for confirming a drift.</p>
<p>SPC可用于<strong>测量</strong>警告和失控之间的<strong>时间变化率</strong>。时间<strong>短</strong>表明<strong>变化快</strong>。距离越长表示变化越慢。<strong>变化率</strong>也可以作为警告期间示例数的比率误差来衡量。 SPC依靠<strong>误差方差的估计来定义动作界限</strong>，动作界限随着误差估计的置信度增加而缩小。 SPC可以在增量学习算法内部实现，也可以作为批处理算法的包装器实现（请参阅在线附录A中的算法3）。<br>        SPC can be used to measure the rate of change as the time between Warning and Out-of-Control. Short times indicate fast changes; longer distances indicate slower changes. The rate of change can also be measured as the rate errors to the number of examples during Warning. SPC relies on the estimates of the error variance to define the action bounds, which shrink as the confidence of the error estimates increases. SPC can be implemented inside incremental learning algorithms or as a wrapper to batch algorithms (see Algorithm 3 in online Appendix A).</p>
<p>指数加权移动平均值（EWMA）算法[Ross等。 [2012]在类似想法方面取得了进步。 EWMA通过<strong>逐步降低旧数据的权重</strong>来计算错误率μt的最新估计值：Z0 =μ0和Zt =（1-λ）Zt-1 +λet，t&gt; 0，其中eti是当前示例中的错误。可以看出，与Xt的分布无关，Ztare的均值和标准偏差为μZt=μt和σZt=?。 λ2-λ（1-（1–λ）2t）σx，其中σxi是<strong>流的标准偏差</strong>。假设在变化点之前μt=μ0。 EWMA估算器Zt在此值附近波动。当发生变化时，μcandZt的μtchange值将通过从μ0向μc分叉对此作出反应。当Zt&gt;μ0+LσZt时，发出变化信号。参数<strong>L（控制极限）</strong>确定发出更改警报之前Zt必须与μ0<strong>相差多远</strong>。<br>        The exponentially weighted moving average (EWMA) algorithm [Ross et al. 2012] advances on similar ideas. The EWMA computes a recent estimate of the error rate, μt, by progressively down-weighting older data: Z0= μ0and Zt= (1 − λ)Zt−1+ λet,t &gt; 0, where etis the error at the current example. It can be shown that, independently of the distribution of Xt, the mean and standard deviation of Ztare μZt= μtand σZt= ? λ 2−λ(1 − (1 − λ)2t)σx, where σxis the standard deviation of the stream. Assume that before the change point μt= μ0. The EWMA estimator Ztfluctuates around this value. When a change occurs, the value of μtchanges to μcand Ztwill react to this by diverging from μ0toward μc. A change is signaled when Zt&gt; μ0+LσZt. The parameter L, the control limit, determines how far Ztmust diverge from μ0before a change alarm is issued.</p>
<p>3.2.3。在<strong>两个不同时间窗口</strong>上监视分发。这些方法通常使用<strong>总结过去信息的固定参考窗口</strong>和<strong>最新示例上的滑动检测窗口</strong>。使用<strong>统计</strong>检验比较这两个窗口上的<strong>分布</strong>，并用<strong>零假设</strong>说明<strong>分布是相等的</strong>。如果null假设被拒绝，则在检测窗口开始时<strong>声明更改</strong>。该窗口可以监视单变量或多变量原始数据（针对每个类别分别监视），以及性能指标的模型参数的变化[Dries and Ruckert 2009]。两个窗口的大小可以<strong>相等</strong>或<strong>递增，</strong>并且可以采用不同的窗口定位策略[Adae和Berthold 2013]。<br>        3.2.3. Monitoring Distributions on T wo Different Time Windows. These methods typically use a fixed reference window that summarizes the past information and a sliding detection window over the most recent examples. Distributions over these two windows are compared using statistical tests with the null hypothesis stating that the distributions are equal. If the null hypothesis is rejected, a change is declared at the start of the detection window . The windows can monitor univariate or multivariate raw data (separately for each class), evolving model parameters of performance indicators [Dries and Ruckert 2009]. The two windows can be of equal or progressive sizes and different window positioning strategies may be employed [Adae and Berthold 2013].</p>
<p>Kifer等人介绍了比较数据流上下文中两个检测窗口上的分布。 [2004]。将来自两个窗口的示例<strong>与基于Chernoff边界的统计检验</strong>进行比较，以确定两个分布是否不同。在同一行中，技术VFDTc [Gama等。通过<strong>连续监视两个示例分布类别之间的差异</strong>：节点为叶时的分布以及该节点的子节点中分布的加权总和，从而具有处理概念漂移的能力。<br>        Comparing distributions on two detection windows in the context of data streams has been introduced in Kifer et al. [2004]. Examples from two windows are compared with statistical tests based on the Chernoff bound to decide whether the two distributions are different. In the same line, the technique VFDTc [Gama et al. 2006] has the ability to deal with concept drift, by continuously monitoring differences between two distribution classes of examples: the distribution when a node was a leaf and the weighted sum of the distributions in the children of that node.</p>
<p>Vorburger和Bernstein [2006]提出了一种<strong>基于熵的度量标准</strong>，用于测量两个滑动窗口之间的分布不平等，分别包括<strong>较旧</strong>的示例和<strong>较新</strong>的示例。<strong>如果分布相等</strong>，则熵度量的结果为1，如果它们<strong>绝对不同</strong>，则度量的结果为<strong>0</strong>。随时间<strong>连续监视熵度量</strong>，并在熵出现时<strong>发出漂移信号</strong>度量值<strong>低于用户定义的阈值</strong>。此类别中的其他变化检测方法[Dasu等。 2006年； Sebastiao和Gama 2007]使用两个不同窗口（旧窗口和最近窗口）的概率分布之间的KullbackLeibler<strong>（KL）散度</strong>来检测可能的变化。<br>        Vorburger and Bernstein [2006] present an entropy-based metric to measure the distribution inequality between two sliding windows including respectively older and more recent examples. If the distributions are equal, the entropy measure results in a value of 1, and if they are absolutely different, the measure will result in a value of 0. The entropy measure is continuously monitored over time, and a drift is signaled when the entropy measure falls below a user-defined threshold. Other change detection methods in this category [Dasu et al. 2006; Sebasti˜ ao and Gama 2007] use the KullbackLeibler (KL) divergence between the probability distributions of two different windows (old and recent) for detecting possible changes.</p>
<p>Bach和Maloof [2008]中提供了一个说明性示例。它使用两种模型：<strong>稳定模型</strong>和<strong>反应</strong>模型。稳定模型的预测基于悠久的历史，而反应模型的预测基于较短的近期时间窗口。该技术使用<strong>反应模型作为概念漂移的指标</strong>，并且使用<strong>稳定模型进行预测</strong>，因为在获取目标概念时，稳定模型的性能优于反应模型。漂移检测方法使用两个模型之间的<strong>精度差异</strong>来确定<strong>何时替换</strong>当前的稳定模型，因为当目标概念发生变化时，稳定模型的性能要比反应模型差。 Nishida和Y amauchi [2007]还考虑了两个精度：在<strong>所有流中估计的精度</strong>和在<strong>最近示例中的滑动窗口上估计的精度</strong>。每当观察到最近精度的显着下降时，就会发出概念漂移的信号。<br>        An illustrative example is presented in Bach and Maloof [2008]. It uses two models: stable and reactive. The stable model predicts based on a long history , whereas the reactive model predicts based on a short recent time window . The technique uses the reactive model as an indicator of concept drift, and it uses the stable model to make predictions, since the stable model performs better than the reactive model when acquiring a target concept. The drift detection method uses the differences in accuracy between the two models to determine when to replace the current stable model, since the stable model performs worse than the reactive model when the target concept changes. Nishida and Y amauchi [2007] also consider two accuracies: the accuracy estimated over all the stream and the accuracy estimated over a sliding window of the most recent examples. A concept drift is signaled whenever a significant decrease in the recent accuracy is observed.</p>
<p>5<br>请注意，检测窗口在概念上与<strong>训练</strong>窗口不同。它们的<strong>大小可能重合</strong>。<strong>检测</strong>窗口用于估计数据分布，<strong>通常是成对的</strong>。训练窗口在生成模型时为学习算法确定训练数据集。<br>        5<br>Note that the detection windows are conceptually different from training windows. They may coincide in size. The detection windows are used for estimating data distribution and are typically paired. The training windows determine the training dataset for the learning algorithm when producing a model.</p>
<p>自适应滑动WINdow或ADWIN [Bifet and Gavalda 2006，2007]是使用检测窗口的另一种变化检测器。令x1，x2，x3，…，xt为实值数据序列。 ADWIN要求输入序列的边界为xt∈[0,1]，只要<strong>固定值的上下边界</strong>，就可以通过<strong>重新缩放原始数据</strong>来实现。将xtw根据Dt绘制的期望值表示为μt； ADWIN不承担任何特定的数据分发。 ADWIN在最近读取的xi上滑动固定的检测窗口W。 L e t ˆμW表示W内示例的（已知）平均值，ndμWμtfort∈W.ADWIN的（未知）平均值在在线附录的算法2中进行了概述，其操作如下。只要W的两个足够大的（子）窗口<strong>表现出足够不同的均值</strong>，该算法就会得出结论，这些窗口内的期望值是不同的，并且会丢弃较旧的（子）窗口。 <strong>Hoeffding边界定</strong>义了足够大和足够明显的地方，测试两个（子）窗口的平均值是否大于？cut计算为？cut：=？ 1 2m·ln4 | W | δ，其中| W |表示（子）窗口的长度；估计| W0 |和| W1 |的谐波平均值，tha s s，m = 2 1 / | W0 | + 1 / | W1 |;dδ∈（0,1）是用户定义的置信度参数，推荐值为0.2。<br>        The ADaptive sliding WINdow or ADWIN [Bifet and Gavalda 2006, 2007] is another change detector using a detection window . Let x1, x2, x3,…,xtbe a sequence of real valued data. ADWIN requires the input sequence to be bounded xt∈ [0,1], which can be achieved by rescaling of the original data as long as the lower and the upper bound of the values are fixed. Denote as μtthe expected value of xtwhen it is drawn according to Dt; ADWIN does not assume any particular data distribution. ADWIN slides a fixed detection window W on the most recently read xi. L e t ˆ μWdenote the (known) average of the examples within W,a n dμWthe (unknown) average ofμtfort ∈ W.ADWINis summarized in Algorithm 2 in the online Appendix, it operates as follows. Whenever two large enough (sub)windows of W exhibit distinct enough means, the algorithm concludes that the expected values within those windows are different, and the older (sub)window is dropped. Large enough and distinct enough are defined by the Hoeffding bound, testing whether the average of the two (sub)windows is larger than ?cutcomputed as ?cut:= ? 1 2m· ln4|W| δ, where |W| denotes the length of a (sub)window; mis the harmonic mean of|W0|and|W1|,t h a ti s ,m= 2 1/|W0|+1/|W1|;a n dδ ∈ (0,1) is a user-definedconfidence parameter , the recommended value for which is 0.2.</p>
<p>与之前讨论的顺序检测器相比，基于监视两个分布的检测方法的主要局限性是内存需求。虽然顺序检测器不需要存储传入的数据，但基于分布的检测器需要在两个窗口内存储数据。主要优点是可以更精确地定位变化点（尽管至少要延迟W个样本）。可以使用指数直方图的变体为ADWIN配备压缩功能[Datar等。 2002]。因此，不需要存储来自检测窗口W的所有示例。它仅将此数据存储在O（logW）内存中，并且每项存储O（logW）处理时间，而不是O（W）。<br>        The main limitation of the detection methods based on monitoring two distributions as compared to sequential detectors discussed before is the memory requirements. While sequential detectors do not need to store the incoming data, the distributionbased detectors need to store data within the two windows. The main advantage is more precise localization of the change point (although with a delay of at least W samples). ADWIN can be equipped with compression using a variant of the exponential histogram [Datar et al. 2002]. Therefore, it does not need to store all the examples from the detection window W; it stores this data in only O(logW) memory and O(logW) processing time per item, rather than the O(W).</p>
<p>接下来讨论的自适应技术最初已与特定的更改检测器结合在一起。通常，一种策略可以部署任何检测器，因为检测器所需的主要信息是是否已发生更改。<br>        The adaptation techniques discussed next have originally been coupled with specific change detectors. In general, a strategy can deploy any detector, since the main information that is needed from a detector is whether a change has occurred or not.</p>
<p>3.2.4。上下文方法。接头系统[Harries等。 [1998]提出了一种元学习技术，该技术实现了上下文相关的批处理学习方法。接合旨在识别具有稳定隐藏上下文的区间，并引入和完善与这些上下文关联的局部概念。这个想法是使用示例的时间戳作为批处理分类器的输入功能。在第一阶段，示例具有时间戳功能，并且学习了决策树。如果决策树在时间戳功能上找到拆分，则该功能上的分区会建议不同的上下文。在第二阶段，将C4.5应用于每个分区以查找临时（时间）概念。<br>        3.2.4. Contextual Approaches. Splice system [Harries et al. 1998] presents a meta-learning technique that implements a context-sensitive batch-learning approach. Splice is designed to identify intervals with stable hidden context and to induce and refine local concepts associated with these contexts. The idea is to use a timestamp of the examples as an input feature to a batch classifier. In the first stage, examples are augmented with a timestamp feature, and a decision tree is learned. If the decision tree finds splits on the timestamp feature, the partitions on that feature suggest different contexts. In the second phase, C4.5 is applied to each partition to find the interim (temporal) concepts.</p>
<p>与是否使用时间窗无关，需要有健壮的方法来在增量学习和遗忘之间取得平衡，以应对不断变化的环境。这个想法被应用在增量模糊分类系统（IFCS）算法中[Bouchachia 2011a]，该算法维护了一组原型。即，每个类由许多群集表示/覆盖。使用了三种机制，称为陈旧性，惩罚性和总体准确性。前两个措施解决了模型复杂性以及逐步漂移的问题。如果两个度量值之一降至非常小的阈值（称为删除阈值）以下，则将删除原型。最后一个用于处理逐渐和突然的漂移。陈旧性会跟踪原型的活动，以做出有关新输入的决策，而这种活动表明最近传入的新输入来自原型所覆盖的输入空间。过时的原型倾向于覆盖过时的区域。为了表示陈旧性机制，使用以下公式：w（i）= ζt−at，其中t和at分别表示当前时间指数和上一次原型i获胜的时间。遗忘因子ζ的较小值可加速重量的减轻。权重与原型相关联。它们对于陈旧原型的价值会下降，而对于现役原型的价值则会增强。显然，如果陈旧时间过长（即t-atis足够大），它将消失，然后原型消失。<br>        Independently of whether a time windowing is used, robust approaches that yield a balance between incremental learning and forgetting are needed to deal with changing environments. This idea was applied in the incremental fuzzy classification system (IFCS) algorithm [Bouchachia 2011a], which maintains a set of prototypes. That is, each class is represented/covered by many clusters. Three mechanisms, calledstaleness, penalization, and overall accuracy, are used. The first two measures tackle the problem of model complexity as well as gradual drift. If one of the two measure values falls below a very small threshold, called removal threshold, the prototype is removed. The last one is intended for handling gradual and abrupt drift. Staleness tracks the activity of the prototype in making decisions about the new input, and such an activity indicates that the recent incoming new input emanates from the input space covered by the prototype. Stale prototypes tend to cover obsolete regions. To express the staleness mechanism, the following formula is used: w(i) = ζt−at, where t and atindicate, respectively , the current time index and the last time the prototype i was a winner . Small values of the forgetting factor ζ accelerate the reduction in the weight. The weights are associated with the prototypes. Their value for stale prototypes decays and for active prototypes it reinforces. Clearly , if the staleness is long (i.e., t − atis large enough), widiminishes and then the prototype vanishes.</p>
<p>第二种机制着重于跟踪决策的准确性，从而根据与最近输入模式的一致性来观察模型的演变。目的是确保精度不降低（至少不会显着降低）。使用以下公式：zi =λsi，其中si是自原型创建以来所产生的错误数量。随着错误数量的增加，权重呈指数下降。 λ值越小，遗忘速度和模型更新越快。第三种机制是统计过程控制（SPC）标准，这将在3.2.2节中进行说明，其目的是根据预测过程中学习模型产生的错误数量来处理逐渐变化和突变。<br>        The second mechanism focuses on tracking the accuracy of the decisions made and thus observing the evolution of the model in terms of consistency with the recent input patterns. The aim is to ensure that the accuracy does not deteriorate (at least significantly).Thefollowingformulahasbeenused: zi= λsi, wheresiis number of errors made by the prototype since it has been created. The weight decreases exponentially as the number of errors increases. The smaller the value of λ, the more quickly are the forgetting speed and the model update. The third mechanism is the statistical process control (SPC) criterion, which is explained in Section 3.2.2 and which aims at handling gradual and abrupt changes based on the number of errors produced by the learning model during prediction.</p>
<p>表II总结了所提出的变更检测算法的属性。总体而言，比较两个分布以进行更改检测比基于指标的演变进行检测需要更多的计算资源，但可能会提供有关更改位置的更精确信息。表III总结了主要的变化检测技术。<br>        Table II summarizes properties of the presented change detection algorithms. Overall, comparing two distributions for change detection requires more computational resources than detecting based on the evolution of indicators, but potentially may give more precise information about the location of change. Table III summarizes the main change detection techniques.</p>
<p>3.3。学习<br>学习组件是指用于从示例中进行<strong>概括</strong>并从不断发展的数据中<strong>更新预测模型</strong>的技术和机制。本节组织如下：（i）<strong>学习模式是</strong>指在有新数据点可用时的模型更新； （ii）<strong>模型适应</strong>分析了随时间变化的数据的预测模型的行为； （iii）<strong>模型管理</strong>是指用于维护主动预测模型的技术。<br>        3.3. Learning<br>The learning component refers to the techniques and mechanisms for generalizing from examples and updating the predictive models from evolving data. This section is organized as follows: (i) learning mode refers to model updating when new data points are available; (ii) model adaptation analyzes the behavior of predictive models on timeevolving data; (iii) model management refers to the techniques for maintaining active predictive models.</p>
<p>3.3.1。学习模式。只要有新的标记示例可用，学习系统就可以更新模型。我们考虑了两种不同的学习模式：<strong>重新训练</strong>（使用现有数据丢弃现有模型并从头开始构建新模型），以及<strong>模型的增量适应更新</strong>。<br>        3.3.1. Learning Mode. Whenever new labeled examples are available, the learning system might update the model. We consider two different learning modes: retraining that discards the current model and builds a new model from scratch using buffered data, and incremental adaptation updates of the model.</p>
<p>训练方法需要一些数据缓冲区存储在内存中。再培训已被用于通过批处理学习算法来<strong>模拟</strong>增量学习[Gama等。 2004]。首先，使用所有可用数据来训练模型。接下来，每当新数据到达时，先前的模型就被丢弃，新数据与先前的数据合并，并从该数据中学习新模型[Street and Kim 2001; Zeira等。 2004; Klinkenberg and Joachims 2000]。<br>        Retraining approaches need some data buffer to be stored in memory . Retraining has been used to emulate incremental learning with batch-learning algorithms [Gama et al. 2004]. At the beginning, a model is trained with all the available data. Next, whenever new data arrives, the previous model is discarded, the new data is merged with the previous data, and a new model is learned on this data [Street and Kim 2001; Zeira et al. 2004; Klinkenberg and Joachims 2000].</p>
<p><strong>增量方法</strong>使用最新数据来更新当前模型。增量算法会逐个处理输入示例，并在<strong>收到每个示例后更新存储在模型中的足够统计信息</strong>。他们可以访问以前的示例或示例摘要。 CVFDT就是这种情况[Hulten等。 [2001年]在下一节中介绍。在线学习模式使用最新示例更新当前模型。它们受错误驱动，根据当前模型是否对当前示例分类错误来更新当前模型。例如WINNOW [Littlestone 1987]和MBW [Carvalho and Cohen 2006]。 MBW [Carvalho and Cohen 2006]指的是众所周知的算法，包括感知器和多层感知器，传统上是使用多次训练数据进行训练的；当仅限于一次训练传递数据时，它们与海量和流式数据特别相关[Carvalho and Cohen 2006]。在线学习的起源可以追溯到1960年代末的Perceptron [Rosenblatt 1958]，和2000年代初的“专家建议下的预测”范式的出现[Cesa-Bianchi and Lugosi 2006]。早期的工作出现在许多开创性的论文中[Littlestone 1987; Littlestone和Warmuth，1994年；赫伯斯特和沃默斯1998； Vovk 1998]。使用当前示例更新模型。随着时间的流逝，新到达的数据趋向于擦除先前的模式。在诸如人工神经网络之类的模型中，学习不可避免地与遗忘联系在一起。在保留以前学到的知识的同时不断从示例流中学习的能力称为“稳定性可塑性难题” [Carpenter等。 1991a]。这是一个难题，因为需要在稳定的噪声处理能力和学习新模式之间取得平衡。当暴露于一组新的模式时，一些人工神经网络会完全忘记过去的模式。这种现象被称为[French 1994; Polikar等。 2001]。<br>        Incremental approaches update the current model using the most recent data. Incremental algorithms process input examples one by one and update the sufficient statistics stored in the model after receiving each example. They might have access to previous examples or summaries of examples. This is the case of CVFDT [Hulten et al. 2001] described in the next section. The online learning mode updates the current model with the most recent example. They are error driven, updating the current model depending on whether it misclassifies the current example. Examples include WINNOW [Littlestone 1987] and MBW [Carvalho and Cohen 2006]. MBW [Carvalho and Cohen 2006] refers to well-known algorithms, includingperceptron and multilayer perceptrons, t h a t have been traditionally trained using several passes through the training data; when restricted to a single training pass over the data they are particularly relevant for massive and streaming data [Carvalho and Cohen 2006]. The origins of online learning go back to the late 1960s, with Perceptron [Rosenblatt 1958], and the early 2000s, with the advent of the paradigm of “prediction with expert advice” [Cesa-Bianchi and Lugosi 2006]. The early work appeared in a number of seminal papers [Littlestone 1987; Littlestone and Warmuth 1994; Herbster and Warmuth 1998; Vovk 1998]. The model is updated with the current example. As time goes on, the newly arrived data tend to erase away the prior patterns. In models, such as artificial neural networks, learning is inevitably connected with forgetting. The ability to continuously learn from a stream of examples while preserving previously learned knowledge is known as the stabilityplasticity dilemma [Carpenter et al. 1991a]. It is a dilemma because there needs to be a balance between being stable to handle noise and being able to learn new patterns. Some artificial neural networks completely forget the past patterns when exposed to a new set of patterns; this phenomenon is known as [French 1994; Polikar et al. 2001].</p>
<p>流算法是用于处理高速连续数据流的在线算法。示例按顺序进行处理，仅需几遍即可检查（通常只有一遍）。这些算法使用有限的内存并控制可用内存。例如，霍夫丁树[Domingos and Hulten 2000]和变体VFDTc [Gama等。 2006]或FIMT-DD [Ikonomovska等。 2011]能够在记忆力不足时冻结树叶。<br>        Streaming algorithms are online algorithms for processing high-speed continuous flow of data. Examples are processed sequentially and can be examined in only a few passes (typically just one). These algorithms use limited memory and control the available memory . For example, Hoeffding trees [Domingos and Hulten 2000] and variants VFDTc [Gama et al. 2006] or FIMT-DD [Ikonomovska et al. 2011] are able to freeze leaves when memory becomes scarce.</p>
<h4 id="3-3-2。适应方法。"><a href="#3-3-2。适应方法。" class="headerlink" title="3.3.2。适应方法。"></a>3.3.2。适应方法。</h4><p>适应策略管理预测模型的适应。它们分为两种主要类型：盲目和知情。<br>        3.3.2. Adaptation Methods. The adaptation strategies manage adaptation of the predictive model. They fall into two major types: blind and informed.</p>
<p>盲目适应策略可在不显式检测更改的情况下适应模型。盲法适应通常使用固定大小的滑动窗口作为参数，将窗口大小w作为参数，并使用最新的w实例定期对模型进行重新训练，而示例权重则考虑了训练实例的重要性[Widmer and Kubat 1996; Klinkenberg和Renz 1998； Klinkenberg和Joachims，2000； Lanquillon 2002]。<br>        The blind adaptation strategies adapt the model without any explicit detection of changes. Blind adaptation typically uses techniques as fixed-size sliding windows that take a window size w as a parameter and periodically retrain the model with the latest w examples, and example weighting that considers the importance of training examples [Widmer and Kubat 1996; Klinkenberg and Renz 1998; Klinkenberg and Joachims 2000; Lanquillon 2002].</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/">http://example.com/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/19/%E8%AE%B2%E5%BA%A7-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">讲座-联邦学习综述</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/18/%E8%AE%BA%E6%96%87001-Federated-Machine-Learning-Concept-and-Applications/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">论文001-Federated_Machine_Learning:_Concept_and_Applications</div></div></a></div></nav></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">John Doe</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">15</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#intro"><span class="toc-number">1.</span> <span class="toc-text">intro</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-2-ADAPTIVE-LEARNING-ALGORITHMS"><span class="toc-number">2.</span> <span class="toc-text">2.自适应学习算法 2. ADAPTIVE LEARNING ALGORITHMS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E3%80%82%E8%AE%BE%E7%BD%AE%E5%92%8C%E5%AE%9A%E4%B9%89-2-1-Setting-and-Definitions"><span class="toc-number">2.1.</span> <span class="toc-text">2.1。设置和定义 2.1. Setting and Definitions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2%E3%80%82%E6%95%B0%E6%8D%AE%E9%9A%8F%E6%97%B6%E9%97%B4%E5%8F%98%E5%8C%96-2-2-Changes-in-Data-Over-Time"><span class="toc-number">2.2.</span> <span class="toc-text">2.2。数据随时间变化 2.2. Changes in Data Over Time</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3%E3%80%82%E4%B8%8D%E6%96%AD%E5%8F%98%E5%8C%96%E7%9A%84%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%AF%B9%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%81%E6%B1%82-2-3-Requirements-for-Predictive-Models-in-Changing-Environments"><span class="toc-number">2.3.</span> <span class="toc-text">2.3。不断变化的环境中对预测模型的要求 2.3. Requirements for Predictive Models in Changing Environments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4%E3%80%82%E5%9C%A8%E7%BA%BF%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F"><span class="toc-number">2.4.</span> <span class="toc-text">2.4。在线自适应学习程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5%E3%80%82%E8%AF%B4%E6%98%8E%E6%80%A7%E5%BA%94%E7%94%A8"><span class="toc-number">2.5.</span> <span class="toc-text">2.5。说明性应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A6%82%E5%BF%B5%E6%BC%82%E7%A7%BB%E9%80%82%E5%BA%94%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%86%E7%B1%BB%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">3.概念漂移适应方法的分类法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E3%80%82%E5%8F%98%E6%9B%B4%E6%A3%80%E6%B5%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.2。变更检测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2%E3%80%82%E9%80%82%E5%BA%94%E6%96%B9%E6%B3%95%E3%80%82"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.3.2。适应方法。</span></a></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2020/11/27/A-survey-of-Methods-for-Time-Series/" title="A_survey_of_Methods_for_Time_Series"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A_survey_of_Methods_for_Time_Series"/></a><div class="content"><a class="title" href="/2020/11/27/A-survey-of-Methods-for-Time-Series/" title="A_survey_of_Methods_for_Time_Series">A_survey_of_Methods_for_Time_Series</a><time datetime="2020-11-27T02:48:30.000Z" title="Created 2020-11-27 10:48:30">2020-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/11/19/%E4%BB%8E%E5%8F%8D%E5%A4%8D%E6%97%A0%E5%B8%B8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0-%E4%B8%80%E7%A7%8D%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95/" title="从反复无常的数据流中进行在线学习:一种生成方法"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从反复无常的数据流中进行在线学习:一种生成方法"/></a><div class="content"><a class="title" href="/2020/11/19/%E4%BB%8E%E5%8F%8D%E5%A4%8D%E6%97%A0%E5%B8%B8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0-%E4%B8%80%E7%A7%8D%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95/" title="从反复无常的数据流中进行在线学习:一种生成方法">从反复无常的数据流中进行在线学习:一种生成方法</a><time datetime="2020-11-19T13:01:07.000Z" title="Created 2020-11-19 21:01:07">2020-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/11/19/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0advances-and-open-problems-in-Federated-learning/" title="联邦学习综述advances_and_open_problems_in_Federated_learning"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="联邦学习综述advances_and_open_problems_in_Federated_learning"/></a><div class="content"><a class="title" href="/2020/11/19/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0advances-and-open-problems-in-Federated-learning/" title="联邦学习综述advances_and_open_problems_in_Federated_learning">联邦学习综述advances_and_open_problems_in_Federated_learning</a><time datetime="2020-11-19T07:30:07.000Z" title="Created 2020-11-19 15:30:07">2020-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/11/19/%E8%AE%B2%E5%BA%A7-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/" title="讲座-联邦学习综述"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="讲座-联邦学习综述"/></a><div class="content"><a class="title" href="/2020/11/19/%E8%AE%B2%E5%BA%A7-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/" title="讲座-联邦学习综述">讲座-联邦学习综述</a><time datetime="2020-11-19T06:20:49.000Z" title="Created 2020-11-19 14:20:49">2020-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/" title="005_A_Survey_on_Concept_Drift_Adaptation"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="005_A_Survey_on_Concept_Drift_Adaptation"/></a><div class="content"><a class="title" href="/2020/11/18/005-A-Survey-on-Concept-Drift-Adaptation/" title="005_A_Survey_on_Concept_Drift_Adaptation">005_A_Survey_on_Concept_Drift_Adaptation</a><time datetime="2020-11-18T08:40:55.000Z" title="Created 2020-11-18 16:40:55">2020-11-18</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 By John Doe</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>